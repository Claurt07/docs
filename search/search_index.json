{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HOT Tech Docs","text":"<p>\ud83d\udcd6 Welcome to the HOT Technical documentation for all of our open-source tools.</p> <p> \ud83d\udc48 Click here to open the sidebar content. </p> <p>This website is an index of tools available from HOT, and it is primarily intended for a software development audience.</p> <p>The docs go into specific technical detail for:</p> <ul> <li>Code structure and higher level overviews to get started.</li> <li>Deployment / installation of software tools, including configuration options.</li> <li>Options and variables for CLI commands.</li> <li>Web API references to better understand expected inputs and responses.</li> <li>Library API references for developing against.</li> </ul> <p>Each sidebar item will link to the documentation specific to each repo.</p> <p>HOT is working towards the modularization of our code to improve maintainability and implement an end-to-end user flow between multiple tools. There is more info about modularization of our code.</p>"},{"location":"#the-hot-ecosystem-of-tools","title":"The HOT Ecosystem of Tools","text":""},{"location":"LICENSE/","title":"GNU AFFERO GENERAL PUBLIC LICENSE","text":"<p>Version 3, 19 November 2007</p> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/</p> <p>Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p>"},{"location":"LICENSE/#preamble","title":"Preamble","text":"<p>The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.</p> <p>When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.</p> <p>A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.</p> <p>The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.</p> <p>An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p>"},{"location":"LICENSE/#terms-and-conditions","title":"TERMS AND CONDITIONS","text":""},{"location":"LICENSE/#0-definitions","title":"0. Definitions","text":"<p>\"This License\" refers to version 3 of the GNU Affero General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p>"},{"location":"LICENSE/#1-source-code","title":"1. Source Code","text":"<p>The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p>"},{"location":"LICENSE/#2-basic-permissions","title":"2. Basic Permissions","text":"<p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.</p>"},{"location":"LICENSE/#3-protecting-users-legal-rights-from-anti-circumvention-law","title":"3. Protecting Users' Legal Rights From Anti-Circumvention Law","text":"<p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p>"},{"location":"LICENSE/#4-conveying-verbatim-copies","title":"4. Conveying Verbatim Copies","text":"<p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p>"},{"location":"LICENSE/#5-conveying-modified-source-versions","title":"5. Conveying Modified Source Versions","text":"<p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <ul> <li>a) The work must carry prominent notices stating that you modified   it, and giving a relevant date.</li> <li>b) The work must carry prominent notices stating that it is   released under this License and any conditions added under   section 7. This requirement modifies the requirement in section 4   to \"keep intact all notices\".</li> <li>c) You must license the entire work, as a whole, under this   License to anyone who comes into possession of a copy. This   License will therefore apply, along with any applicable section 7   additional terms, to the whole of the work, and all its parts,   regardless of how they are packaged. This License gives no   permission to license the work in any other way, but it does not   invalidate such permission if you have separately received it.</li> <li>d) If the work has interactive user interfaces, each must display   Appropriate Legal Notices; however, if the Program has interactive   interfaces that do not display Appropriate Legal Notices, your   work need not make them do so.</li> </ul> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p>"},{"location":"LICENSE/#6-conveying-non-source-forms","title":"6. Conveying Non-Source Forms","text":"<p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <ul> <li>a) Convey the object code in, or embodied in, a physical product   (including a physical distribution medium), accompanied by the   Corresponding Source fixed on a durable physical medium   customarily used for software interchange.</li> <li>b) Convey the object code in, or embodied in, a physical product   (including a physical distribution medium), accompanied by a   written offer, valid for at least three years and valid for as   long as you offer spare parts or customer support for that product   model, to give anyone who possesses the object code either (1) a   copy of the Corresponding Source for all the software in the   product that is covered by this License, on a durable physical   medium customarily used for software interchange, for a price no   more than your reasonable cost of physically performing this   conveying of source, or (2) access to copy the Corresponding   Source from a network server at no charge.</li> <li>c) Convey individual copies of the object code with a copy of the   written offer to provide the Corresponding Source. This   alternative is allowed only occasionally and noncommercially, and   only if you received the object code with such an offer, in accord   with subsection 6b.</li> <li>d) Convey the object code by offering access from a designated   place (gratis or for a charge), and offer equivalent access to the   Corresponding Source in the same way through the same place at no   further charge. You need not require recipients to copy the   Corresponding Source along with the object code. If the place to   copy the object code is a network server, the Corresponding Source   may be on a different server (operated by you or a third party)   that supports equivalent copying facilities, provided you maintain   clear directions next to the object code saying where to find the   Corresponding Source. Regardless of what server hosts the   Corresponding Source, you remain obligated to ensure that it is   available for as long as needed to satisfy these requirements.</li> <li>e) Convey the object code using peer-to-peer transmission,   provided you inform other peers where the object code and   Corresponding Source of the work are being offered to the general   public at no charge under subsection 6d.</li> </ul> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p>"},{"location":"LICENSE/#7-additional-terms","title":"7. Additional Terms","text":"<p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <ul> <li>a) Disclaiming warranty or limiting liability differently from the   terms of sections 15 and 16 of this License; or</li> <li>b) Requiring preservation of specified reasonable legal notices or   author attributions in that material or in the Appropriate Legal   Notices displayed by works containing it; or</li> <li>c) Prohibiting misrepresentation of the origin of that material,   or requiring that modified versions of such material be marked in   reasonable ways as different from the original version; or</li> <li>d) Limiting the use for publicity purposes of names of licensors   or authors of the material; or</li> <li>e) Declining to grant rights under trademark law for use of some   trade names, trademarks, or service marks; or</li> <li>f) Requiring indemnification of licensors and authors of that   material by anyone who conveys the material (or modified versions   of it) with contractual assumptions of liability to the recipient,   for any liability that these contractual assumptions directly   impose on those licensors and authors.</li> </ul> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p>"},{"location":"LICENSE/#8-termination","title":"8. Termination","text":"<p>You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p>"},{"location":"LICENSE/#9-acceptance-not-required-for-having-copies","title":"9. Acceptance Not Required for Having Copies","text":"<p>You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p>"},{"location":"LICENSE/#10-automatic-licensing-of-downstream-recipients","title":"10. Automatic Licensing of Downstream Recipients","text":"<p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p>"},{"location":"LICENSE/#11-patents","title":"11. Patents","text":"<p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p>"},{"location":"LICENSE/#12-no-surrender-of-others-freedom","title":"12. No Surrender of Others' Freedom","text":"<p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p>"},{"location":"LICENSE/#13-remote-network-interaction-use-with-the-gnu-general-public-license","title":"13. Remote Network Interaction; Use with the GNU General Public License","text":"<p>Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.</p> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.</p>"},{"location":"LICENSE/#14-revised-versions-of-this-license","title":"14. Revised Versions of this License","text":"<p>The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p>"},{"location":"LICENSE/#15-disclaimer-of-warranty","title":"15. Disclaimer of Warranty","text":"<p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p>"},{"location":"LICENSE/#16-limitation-of-liability","title":"16. Limitation of Liability","text":"<p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>"},{"location":"LICENSE/#17-interpretation-of-sections-15-and-16","title":"17. Interpretation of Sections 15 and 16","text":"<p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"LICENSE/#how-to-apply-these-terms-to-your-new-programs","title":"How to Apply These Terms to Your New Programs","text":"<p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as\n    published by the Free Software Foundation, either version 3 of the\n    License, or (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.</p>"},{"location":"code-of-conduct/","title":"\ud83d\udcdc Code of conduct","text":"<p>(The latest version can be found at https://www.hotosm.org/code-of-conduct</p> <p>Welcome to Humanitarian OpenStreetMap Team. HOT is committed to providing a welcoming and safe environment for people of all races, gender identities, gender expressions, sexual orientations, physical abilities, physical appearances, socio-economic backgrounds, nationalities, ages, religions, and beliefs.</p> <p>The HOT community principles are:</p> <ul> <li>Be friendly and patient. Be generous and kind in both giving and accepting   critique. Critique is a natural and important part of our culture. Good   critiques are kind, respectful, clear, and constructive, focused on goals and   requirements rather than personal preferences. You are expected to give and   receive criticism with grace. Be considerate in speech and actions, and   actively seek to acknowledge and respect the boundaries of fellow attendees.</li> </ul> <ul> <li> <p>Be welcoming. We strive to be a community that welcomes and supports   people of all backgrounds and identities. Some examples of behavior that   contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language.</li> </ul> <ul> <li>Being respectful of differing viewpoints and experiences.</li> </ul> <ul> <li>Gracefully accepting constructive criticism.</li> </ul> <ul> <li>Showing empathy towards other community members.</li> </ul> <ul> <li>Placing collective interest before your own interest.</li> </ul> </li> </ul> <ul> <li>Be considerate. Your work will be used by other people, and you in turn   will depend on the work of others. Any decision you take will affect users and   colleagues, and you should take those consequences into account when making   decisions. Remember that we're a world-wide community, so you might not be   communicating in someone else's primary language.</li> </ul> <ul> <li>Be respectful. Not all of us will agree all the time, but disagreement is   no excuse for poor behavior and poor manners. We might all experience some   frustration now and then, but we cannot allow that frustration to turn into a   personal attack. It\u2019s important to remember that a community where people feel   uncomfortable or threatened is not a productive one. Members of the HOT   community should be respectful when dealing with other members as well as with   people outside the HOT community.</li> </ul> <ul> <li> <p>Be careful in your word choice. We are a global community of   professionals, and we conduct ourselves professionally. Be kind to others. Do   not insult or put down other participants. Harassment and other exclusionary   behavior aren't acceptable. This includes, but is not limited to:</p> <ul> <li>Violent threats or language directed against another person.</li> </ul> <ul> <li>Discriminatory jokes and language.</li> </ul> <ul> <li>Posting sexually explicit or violent material.</li> </ul> <ul> <li>Posting (or threatening to post) other people's personally identifying   information (\"doxing\").</li> </ul> <ul> <li>Personal insults, especially those using racist or sexist terms.</li> </ul> <ul> <li>Unwelcome sexual attention.</li> </ul> <ul> <li>Advocating for, or encouraging, any of the above behavior.</li> </ul> <ul> <li>Repeated harassment of others. In general, if someone asks you to stop, then   stop.</li> </ul> </li> </ul> <ul> <li>Assume all communications are positive. Always remain polite, and assume   good faith. It is surprisingly easy to misunderstand each other, be it online   or in person, particularly in such a culturally diverse setting as ours.   Misunderstandings are particularly easy to arise when we are in a rush, or   otherwise distracted. Please ask clarifying questions before assuming that a   communication was inappropriate.</li> </ul> <ul> <li>When we disagree, try to understand why. Disagreements, both social and   technical, happen easily and often. It is important that we resolve such   disagreements and differing views constructively. At times it can be hard to   appreciate a viewpoint that contradicts your own perceptions. Instead of pushing   back, try to understand where the other person is coming from, and don\u2019t be   afraid to ask questions. You can be most helpful if your own replies serve to   clarify, rather than to escalate an issue. Also don\u2019t forget that it can be   easy to make mistakes, and allow for the possibility that the mistake may have   been yours. When this happens it is better to resolve the issue together, and   to learn from the experience together, than to place blame.</li> </ul> <p>Original text courtesy of the Speak Up! project.</p> <p>Further sources:</p> <ul> <li>Ada Initiative: HOWTO design a code of conduct for your community</li> </ul> <ul> <li>Algorithm Club Code of Conduct</li> </ul> <ul> <li>American Red Cross GIS Team Code of Conduct</li> </ul> <ul> <li>Contributor Covenant \u2013 A Code of Conduct for Open Source Projects</li> </ul> <ul> <li>Django Code of Conduct</li> </ul> <ul> <li>Mozilla Community Participation Guidelines</li> </ul> <ul> <li>Vox Media Code of Conduct</li> </ul>"},{"location":"code-of-conduct/#complaint-handling-process","title":"Complaint Handling Process","text":"<p>As a first measure, it is preferable to work out issues directly with the people involved, or to work with other Community Members who can help you resolve the issue. This may take several forms:</p> <ul> <li>Talk with one another. Assume that communications are positive and that people   are treating each other with respect. Cues about emotions are often lacking   from digital communications. Many of our modes of digital communication tend   towards brevity, which can be easier to interpret incorrectly as being negative.</li> </ul> <ul> <li>Contact a representative of the Community Working Group, which exists to   support the HOT Community. Representatives are available to discuss any   concerns about behaviour within the community, or ideas to promote positive   behaviours. You can email them at   community@hotosm.org.</li> </ul> <ul> <li>Contact a representative of the Governance Working Group, which drafted   these recommendations and the CoC. Representatives are available to provide   advice on particular scenarios, as well as on the processes around the CoC.</li> </ul> <ul> <li>Contact the HOT Chair of Voting Members.</li> </ul> <ul> <li>Contact a HOT Board Member. Board members are well versed in the   community and its management. They can offer advice on your particular   situation, and know the resources of the organization that may be available to   you.</li> </ul> <ul> <li>Contact the HOT Community Partnerships Manager.</li> </ul> <p>When these informal processes fail, or when a situation warrants an immediate response by HOT, you can evoke the HOT Policy and Code of Conduct Complaint Handling Process. This process was adopted by HOT Voting Members in 2016 to provide a more formal means of enforcement for our community standards. You start it by emailing complaints@hotosm.org with a description of your complaint, your name, and the name of the offending party. All complaints will be considered confidential. The full process is described here .</p>"},{"location":"e2e-overview/","title":"End-to-End Overview","text":"<p>This is an overview of the proposed end-to-end workflow that ties together all of HOT's tools.</p> <p>Outcome: to enable communities to generate their own maps for their area of interest (neighbourhood, city, region), rich in data derived from local knowledge.</p>"},{"location":"e2e-overview/#requirements","title":"Requirements","text":"<ul> <li>A project owner / coordinator who can pull all of the pieces together.</li> <li>Users in the area of interest with drones, and willing to produce imagery.<ul> <li>This has typically been a barrier, but with decent cheap consumer drones   being available now, the opportunity to produce high-resolution base imagery   has opened up.</li> </ul> </li> <li>A small team of digitisers, who use TM to generate a traning dataset for AI   models.<ul> <li>This could feasibly be remote, but ideally local for better knowledge of   what the reality is on the ground.</li> </ul> </li> <li>A group of local people who can verify features on the ground, adding tags to them   with their local knowledge, via an easy to fill out mobile survey.</li> </ul>"},{"location":"e2e-overview/#methodology-diagram","title":"Methodology (Diagram)","text":""},{"location":"e2e-overview/#methodology-description","title":"Methodology (Description)","text":"<p>\ud83d\udea7 denotes 'blockers' or unknowns in the current workflow.</p>"},{"location":"e2e-overview/#1-collect-drone-imagery","title":"1. Collect Drone Imagery","text":"<ul> <li>Fly drones in grids to collect imagery for the entire area.</li> <li>This will use Drone Aerial Tasking Manager (DATM), if it comes to fruition.</li> </ul> <p>\ud83d\udea7 If DATM succeeds.</p>"},{"location":"e2e-overview/#2-process-a-base-map-upload-to-oam","title":"2. Process a Base Map &amp; Upload to OAM","text":"<ul> <li>Get the imagery from the drones, and preprocess on a drive using EXIF info.</li> <li>Use Node Open Drone Map (NodeODM) to merge into an mosaic.</li> <li>Upload the mosaic to OAM.</li> <li>A TMS URL is generated for you.</li> </ul>"},{"location":"e2e-overview/#3-digitise-features-in-tasking-manager","title":"3. Digitise Features in Tasking Manager","text":"<ul> <li>Load the OAM imagery as a TMS into Tasking Manager.</li> <li>Use a sandboxed version of TM for producing training data.</li> <li>Only map a small sample area that is representative of the entire area.</li> </ul> <p>\ud83d\udea7 We need an instance of TM isolated from OSM, so we don't mess with existing data. Alternatively we need a way to bulk delete geoms via iD Editor.</p>"},{"location":"e2e-overview/#4-generate-a-model-using-fair","title":"4. Generate a Model Using fAIr","text":"<ul> <li>Load the mapped features into fAIr and generate a model.</li> <li>Use the model to predict the remaining features in the area.</li> </ul>"},{"location":"e2e-overview/#5-validate-predicted-features-in-tm","title":"5. Validate Predicted Features in TM","text":"<ul> <li>Load the predicted features back into a TM project.<ul> <li>fAIr allows for feedback to be given to the model, but this cannot be done   collaboratively.</li> </ul> </li> <li>Collaboratively split the geoms as needed, and provide feedback on the generated   geoms.</li> </ul> <p>\ud83d\udea7 We should assess if we need this step, or the input from a single experienced validator directly in fAIr would be good enough.</p> <p>\ud83d\udea7 Can iD Editor be used to split buildings nicely? Is this a good workflow?</p>"},{"location":"e2e-overview/#6-process-final-features-in-fair","title":"6. Process Final Features in fAIr","text":"<ul> <li>Load the corrected features in fAIr.</li> <li>The data could be used to re-train the model.<ul> <li>This process of fAIr--&gt;TM--&gt;fAIr could be done multiple times.</li> </ul> </li> <li>Output the final validated features as a GeoJSON.</li> </ul>"},{"location":"e2e-overview/#7-input-into-fmtm","title":"7. Input Into FMTM","text":"<ul> <li> <p>Now we have validated, AI-generated features for an area, they can be field   validated and tagged.</p> <p>Note the features are not yet in OSM, as they will be field validated first.</p> </li> </ul> <ul> <li>As part of a field (mobile) survey, the features will be validated and tagged with   useful information.</li> <li>The final validated and tagged geometries will be bulk uploaded to OSM.</li> </ul>"},{"location":"e2e-overview/#8-extract-from-opensreetmap","title":"8. Extract from OpenSreetMap","text":"<ul> <li>Now final validation from FMTM is uploaded to OpenStreetMap , Now we need to download them in useful fileformats that we want </li> <li>This will be achieved with HOT Export Tool which allows us to export the data we created in different file formats including ( Shpaefile , Geojson , KML , CSV , Flatgeobuff and manymore ) </li> </ul> <p>\ud83d\udea7 A way to feed validation failures back to TM for re-digitisation would be great.</p>"},{"location":"modules/","title":"Modularization","text":"<p>HOT has many tools which have similar requirements for backend and frontend functionality. To reduce long-term maintainance and code duplication, it's better to have the shared functionality in standalone modules. The other advantage of small sharable modules is it's much easy to enhance or debug when not buried in much larger prohects.</p>"},{"location":"modules/#backend","title":"Backend","text":"<p>Backend Python modules are released on PyPi to be installed across multiple services.</p> <p>See the backend page for more info.</p> <p>Here's a recent Presentation that covers the backend modules in more detail.</p>"},{"location":"modules/#frontend","title":"Frontend","text":"<p>As a form of standardization, we use React as our frontend framework.</p> <p>Many services have common UI components that can be shared (headers, buttons, sidebars, etc).</p> <p>We also use frontend map libraries extensively (obviously).</p> <p>Currently we favour OpenLayers due to it's breadth of functionality.</p> <p>See the frontend page for more info.</p>"},{"location":"roadmap/","title":"Brief Roadmap","text":"<p>The HOT Tech Team is working to improve the user experience across multiple tools, and working towards an end to end data flow between projects. Since often a disaster response, or humanitarian mapping campaign, a project manager or mapper may use the Tasking Manager, Export Tool, fAIr,  FMTM, etc... For more information, here's a short E2E Presentation.</p> <p>To improve maintainability of the code base, the Text Team has been working on modularizing all of our projects. Rather than duplicating functionality used by multiple projects, that functionality has been migrated to standalone python modules so they can be shared. There is more detailed information on modularization here.</p>"},{"location":"roadmap/#block-model-diagram","title":"Block Model Diagram","text":""},{"location":"roadmap/#overall-architecture","title":"Overall Architecture","text":""},{"location":"tools-summary/","title":"HOTOSM Tools","text":""},{"location":"tools-summary/#data-types","title":"Data Types","text":"<p>There are two main types of geospatial data we must consider for maps.</p>"},{"location":"tools-summary/#raster","title":"Raster","text":"<p>This is imagery data - a picture (jpeg, png, tiff, etc).</p> <p>On the lowest level the data is represented by pixels.</p> <p>The pixels are assigned a value within a band:</p> <ul> <li>Single band: the image could be greyscale, where the pixel value   contains a level of gray on a spectrum from white-black.</li> <li>Multi band: most images encountered in the OSM world will contain three   bands (RGB), which can be imagined to be stacked on top of one another.   Red, green, and blue are primary colours than when combined can form   any other colour.</li> </ul> <p>We are mostly interested in multi-band optical imagery, i.e. what you may see on Google / Bing Satellite.</p> <p>This underpins the usage of other data, by visually locating it in space - it tells you where your data is!</p>"},{"location":"tools-summary/#vector","title":"Vector","text":"<p>This is point, polygon, and line data - what you find on OSM!</p> <p>The shapes are represented by some complex maths underneath and rendered on your screen.</p> <p>This is the data that is typically layered on top of basemaps (either raster or vector-based basemaps) for usage.</p>"},{"location":"tools-summary/#how-is-this-data-used","title":"How is this data used?","text":""},{"location":"tools-summary/#input-data","title":"Input Data","text":"<ul> <li>Raster Basemaps:<ul> <li>Google, Bing, ESRI, Mapbox et al all provide web basemaps we can use.</li> <li>Open Aerial Map (OAM) provides more bespoke basemaps of particular areas   of interest (AOIs), over a certain time period. Often higher resolution.</li> </ul> </li> <li>Vector Data:<ul> <li>OpenStreeMap (OSM) is the biggest reference of open map data.<ul> <li>HOT's Export Tool &amp; Raw Data API provide easy access to this.</li> </ul> </li> <li>Any other vector data in various formats:<ul> <li>GIS layers provided by governments: region boundaries, roads, etc.</li> <li>Data collected by NGOs and other organizations, open or not.</li> </ul> </li> </ul> </li> </ul> <p>All of this data can be used for various purposes, involving processing and data analysis.</p>"},{"location":"tools-summary/#processed-data","title":"Processed Data","text":"<p>Generally always vector data in our context.</p> <p>It's OSM vector data that has been processed and packaged in a certain way so it can be used by other tools / users.</p> <p>Using the input data described above, we want to produce data with additional value:</p> <ul> <li>To generate new data to feed back into OSM (a complete loop).</li> <li>For other purposes such as humanitarian response, data analysis   and reporting.</li> </ul>"},{"location":"tools-summary/#hots-tools","title":"HOT's Tools","text":"<p>With the above as context, HOT's tools roughly can be categorised as such:</p>"},{"location":"tools-summary/#input","title":"Input","text":"Tool Description OAM Get base imagery. Raw Data API Extract data from OSM easily for software. Underpass (OSM) Assure data quality. <p>Note: input may be into our own tools, or workflows of others.</p> <p>Each tool is fully open to use by the public.</p>"},{"location":"tools-summary/#output","title":"Output","text":"Tool Description TM Digitize map features remotely. fAIr Speed up the remote digitization process FMTM Add extra information to digitised features in the field. Export Tool Extract data from OSM easily for humans."},{"location":"tools-summary/#openaerialmap-oam","title":"OpenAerialMap (OAM)","text":"<p>OAM should underpin all of our tools.</p> <p>Integrations into other tools allow for basemaps to be loaded to better inform mapping.</p> <p>The thing with satellite imagery is that you have lots of different resolutions available, through different providers.</p> <p>You need to use a mix of providers and decide on the best quality usable for mapping.</p> <p>Many providers (ESRI, Bing, etc) provide free base maps we can use, but OAM should be more targeted - the highest resolution imagery we can get over a time period of interest, for a specific AOI that may be used in TM or FMTM.</p>"},{"location":"tools-summary/#raw-data-api-underpass","title":"Raw Data API &amp; Underpass","text":"<p>They use an innovative database structure to make OSM data much more usable. The idea to make OSM data more accessible, searchable / filterable, and create extracts in formats that can be consumed either by software or users.</p> <p>Underpass is a quality control tool that will sit on top of Raw Data API.</p>"},{"location":"tools-summary/#export-tool","title":"Export Tool","text":"<p>The purpose of Export Tool is to:</p> <ul> <li>Take the users input for what data they need, over what area.</li> <li>Calls Raw Data API to extract and filter the data.</li> <li>Receives back the data in the user requested format, e.g. a JSON   for use in another tool, or a geopackage for use in GIS software by a user.</li> </ul>"},{"location":"tools-summary/#tasking-manager","title":"Tasking Manager","text":"<p>Creating additional vector data to go into OSM, using raster imagery as a source.</p> <p>Pretty much creating polygons and lines from things you and see on a map and uploading them to OSM.</p> <p>The mapping is crowdsourced, or organised by NGOs.</p> <p>Resulting data needs to be validated.</p> <p>Another tool, MapSwipe, can help with this &amp; may be part of the solution. However, TM allows mapping to be done collaboratively.</p>"},{"location":"tools-summary/#fair","title":"fAIr","text":"<p>TM is labour intensive. fAIr uses localised training data to generate a model that can be used to predict vector features from a raster image.</p> <p>Integration into TM means mapping of features can be a lot faster.</p>"},{"location":"tools-summary/#fmtm","title":"FMTM","text":"<p>Adding useful tags, in the field, from the vector data created in TM.</p> <p>The tags provide extra information about what features actually are in OSM.</p> <p>Again, the mapping can be crowdsourced, but more likely organised by NGOs / govs.</p> <p>The FMTM mapping could also provide a feedback loop to TM, helping to validate that features were mapped correctly.</p> <p>As with TM, another tool, StreeComplete, can help with this &amp; may be part of the solution.</p> <p>However, FMTM allows mapping to be done collaboratively.</p>"},{"location":"dev-guide/containers-101/","title":"Containers Intro","text":""},{"location":"dev-guide/containers-101/#why-use-containers","title":"Why Use Containers","text":""},{"location":"dev-guide/containers-101/#the-traditional-setup","title":"The Traditional Setup","text":"<ul> <li>Host machine resources divided up into multiple virtual machines.</li> <li>Each virtual machine running an application.</li> <li>This has no ability to scale resources on the virtual machines:<ul> <li>At times of heavy load the machine resources are limited.</li> <li>At times of low load the resources are under-utilised.</li> </ul> </li> </ul>"},{"location":"dev-guide/containers-101/#the-new-way","title":"The New Way","text":"<ul> <li>Containers remove the need for virtual machines, by providing the   isolation required between applications while retaining access to the   essential low level operating system components on the main machine.<ul> <li>Instead of multiple separate operating systems running, a single   operating system is required. Much more efficient.</li> </ul> </li> </ul>"},{"location":"dev-guide/containers-101/#cool-what-can-i-use-them-for","title":"Cool. What can I use them for?","text":"<ul> <li>Managing dependencies: isolating your code and dependencies from the   host operating system allows you to have different versions of   software installed in different containers.</li> <li>Running software without an install: for example the utility <code>rclone</code>   can be used from it's container image (a pre-packaged environment,   with all required dependencies: docker.io/rclone/rclone), instead of   installing directly on your machine. Clean.</li> <li>Packaging your code for distribution or deployment: written a   backend in Django? It can be packaged up into an image and deployed   anywhere that has a container engine. Your laptop, local server,   AWS, Azure\u2026 you name it.</li> </ul> <p>Containers are now the de-facto way to distribute software. Software developers are required to know the basics of what they are, how to build an image, and how to use a container.</p>"},{"location":"dev-guide/containers-101/#definitions","title":"Definitions","text":"<p>Note: 'Docker' is often used in place of the word 'Container', as this was the main project to popularise containers.</p> <ul> <li>Container (Docker Image): essentially a frozen state of an   operating system, including filesystem, built-in command line tools,   and your application code. This is built from a series of build   instructions, almost exactly how you would have deployed your   application onto a virtual machine.</li> <li>Container: a container image is used to create a running container.   When you run a container, you may wish to specify a network to attach   to, files to mount into the container, and other things.</li> <li>Volume: when a container is shut down, the filesystem is normally   lost - it is ephemeral. A volume allows you to keep data after   the containers lifecycle.</li> </ul>"},{"location":"dev-guide/containers-101/#docker-vs-kubernetes-vs-other","title":"Docker vs Kubernetes vs Other","text":"<ul> <li>In the graphic above, Docker would be the Container Engine. It is   what actually executes the commands to run the container and keep it running.</li> <li>Docker runs on your local machine with single containers.   Docker Inc made a product called Docker Swarm, to allow for the   management of containers across a fleet of servers.   It essentially lost the battle to a Google-backed tool called Kubernetes.</li> <li>Kubernetes is a container orchestration tool and now the standard for   how businesses deploy their software, in a way that is resilient to   server crashes, code logic errors, etc.<ul> <li>If a process fails on one server, e.g. a Django API server, it   will automatically be replaced by an equivalent container on another server.</li> </ul> </li> </ul>"},{"location":"dev-guide/containers-101/#show-me-the-code","title":"Show Me the Code","text":""},{"location":"dev-guide/containers-101/#running-containers","title":"Running containers","text":"<p>https://docs.docker.com/engine/reference/commandline/run/</p> <ul> <li>Run a simple Ubuntu container, based off the Ubuntu Focal image:</li> </ul> <pre><code>docker run -it docker.io/ubuntu:focal bash\n</code></pre> <ul> <li>The <code>-it</code> flag is to tell docker to open an interactive (<code>i</code>) terminal   (<code>t</code>) for you to type commands into the container.</li> <li>The command after the image name is simply <code>bash</code>, which runs a bash   terminal (as opposed to a basic shell terminal: <code>sh</code>).</li> <li>The other mode to run containers is detached (<code>-d</code>), but for this you   need a process to run, instead of a terminal session, for example   <code>python /app/code/main.py</code>.</li> </ul>"},{"location":"dev-guide/containers-101/#building-images","title":"Building Images","text":"<p>Two components are required here:</p> <ul> <li>A Dockerfile (Containerfile). This contains the commands, in order,   that install the dependencies from base image (e.g. Ubuntu), then add   your application code into the image.</li> <li>A build instruction. The command line instruction to build an image,   giving it a name etc:   https://docs.docker.com/engine/reference/commandline/build/</li> </ul> <p>An example Dockerfile:</p> <pre><code># Use a base image with pre-installed dependencies\nFROM node:18-alpine\n# Set the directory to run commands in\nWORKDIR /app\n# Copy your code into the container image\nCOPY . .\n# Install your node dependencies to run the app\nRUN yarn install --production\n# Command to execute at container start (i.e. run a server)\nCMD [\"node\", \"src/index.js\"]\n</code></pre>"},{"location":"dev-guide/containers-101/#additional-references","title":"Additional References","text":"<p>Example build: https://docs.docker.com/get-started/02_our_app/</p> <p>Lots of good tutorials can be found online, search for: Dockerfile build example / tutorial.</p>"},{"location":"dev-guide/containers-cheat-sheet/","title":"Containers Cheat Sheet","text":""},{"location":"dev-guide/dep-management/","title":"Dependency Management","text":""},{"location":"dev-guide/dep-management/#javascript","title":"Javascript","text":"<p>This section will be brief.</p> <p>Javascript already has a great dependency management ecosystem.</p> <p>Tools like NPM, Yarn, PNPM all work in a Node environment to solve dependency version compatibility and install them locally within your repo, in a <code>node_modules</code> directory.</p> <p>The only recommendation would be to use PNPM, as it has a much neater concept then the other two tools. Dependency locking is fast, and package installation is shared across repositories using a global package store on your system.</p> <p>This sets the standard for dependency management that some Python tools have attempted to replicate.</p>"},{"location":"dev-guide/dep-management/#python","title":"Python","text":""},{"location":"dev-guide/dep-management/#tldr","title":"tl;dr","text":"<ul> <li>Use <code>pyproject.toml</code> over <code>requirements.txt</code> and separate config files.</li> <li>Use PDM to solve the depenedencies for your packages.</li> <li>Lock with <code>&gt;=</code> for libraries, and <code>==</code> for applications.</li> <li>Upper bound caping should not be used, e.g. <code>&lt;=x.x.x</code>.</li> </ul>"},{"location":"dev-guide/dep-management/#using-pyprojecttoml","title":"Using pyproject.toml","text":"<p>As per PEP 621, most information about a Python package should live in a file <code>pyproject.toml</code> in the root of your package (often the root of the repo, except in a monorepo setup).</p> <p>This file contains information on required dependencies (no more <code>requirement.txt</code> files), configuration for build tool, linters, test suites (pytest), etc.</p>"},{"location":"dev-guide/dep-management/#dependency-solvers","title":"Dependency Solvers","text":"<p>PDM is recommended.</p> <p>TODO add extra info.</p>"},{"location":"dev-guide/dep-management/#locking-dependency-versions","title":"Locking Dependency Versions","text":"<p>Generally it is good practice to pin the version of an underlying dependency you use in your code. This helps to prevent future breakage.</p> <p>However, there is an important distinction between two types of package:</p> <ul> <li>Libraries: An underlying Python module that is used within another tool.</li> <li>Applications: A software tool. Typically not installable.   Such as HOT's web APIs that underpin it's tools   (raw-data-api, TM, FMTM, etc).</li> </ul> <p>Capping upper limits for library dependencies has long term negative effects, and should never be taken lightly.</p>"},{"location":"dev-guide/dep-management/#locking-for-applications","title":"Locking for Applications","text":"<p>An application would sit at the highest level in the chain of installed dependencies: it uses underlying libraries/packages, but is not installed itself.</p> <p>There are three main options for pinning.</p>"},{"location":"dev-guide/dep-management/#specific-versions","title":"Specific Versions \ud83d\udc4d","text":"<pre><code>pip install mydep==1.0.4\n</code></pre> <p>This is generally the recommended approach for reproducable environments.</p>"},{"location":"dev-guide/dep-management/#approximate-versions","title":"Approximate Versions","text":"<pre><code>pip install mydep~=1.0.4\n</code></pre> <p>This will install &gt;1.0.4, but not increment the minor version.</p> <p>So the maximum installable version here would be 1.0.11, if this is the last version before the 1.2.x minor increment.</p> <p>This is an acceptable approach for some dependencies, but generally not recommended, as SEMVER is no guarantee of avoiding breakage (definitions are quite subjective, so patch versions can sometimes also introduce breakage).</p>"},{"location":"dev-guide/dep-management/#minimum-versions","title":"Minimum Versions","text":"<pre><code>pip install mydep&gt;=1.0.4\n</code></pre> <p>This will install any version greater than that specified.</p> <p>This means breaking changes may be introduced if v2.0.0 is released.</p> <p>This is not recommended for applications, as installing one day may work, then break the next day due a dependency update.</p>"},{"location":"dev-guide/dep-management/#locking-for-libraries-packages","title":"Locking for Libraries / Packages","text":"<p>Sometimes we develop a package that is used as a dependency in other tools.</p> <p>Examples would be:</p> <ul> <li>osm-login-python</li> <li>osm-fieldwork</li> <li>osm-rawdata</li> <li>fmtm-splitter</li> </ul> <p>In these cases, the packages requires underlying dependencies to function.</p> <p>The packages should never have pinned dependencies to a specific version.</p> <p>It is recommended that versions should be pinned in an open ended way, using greater than or equal too (&gt;=).</p> <p>For this to work, a minimum required version of a dependency should be established. There is little point pinning &gt;= if a very up to date version of a dependency is used (forcing the installer to update to a very recent version).</p> <p>This ensures that a minimum version of the dependency is used, but does not prevent dependency upgrades for those using the package.</p> <p>Using approximate pinning (~=) may also be possible, however, this assumes too much about future compatibility, which is something that is difficult to predict.</p> <p>For longevity, it is best to provide the most flexible option for dependency solvers: &gt;=.</p> <p>Only add a cap if a dependency is known to be incompatible or there is a high (&gt;75%) chance of it being incompatible in its next release. An example of a library that should probably be capped is GDAL.</p> <p>Some more in depth technical reading can be found here.</p>"},{"location":"dev-guide/doc-gen/","title":"Auto Documentation Generation","text":""},{"location":"dev-guide/doc-gen/#python","title":"Python","text":""},{"location":"dev-guide/doc-gen/#javascript","title":"Javascript","text":""},{"location":"dev-guide/git/","title":"Git and Pre-Commit Hooks","text":""},{"location":"dev-guide/git/#git-the-basics","title":"Git - The Basics","text":"<p>Is is a tool we all know and love.</p> <p>There are a few basic concepts that would be good to master.</p>"},{"location":"dev-guide/git/#development-lifecycle","title":"Development Lifecycle","text":"<p>The concept of Continuous Integration means that code is continually developed and merged into a branch, for testing or deployment.</p> <p>CI/CD:</p> <ul> <li>CI: merge frequently into master.<ul> <li>Minimise merge issues from multiple collaborators.</li> </ul> </li> <li>CD: deploy master frequently to production.<ul> <li>Regular updates for new features and bug fixes.</li> </ul> </li> </ul> <p>The primary branches, in order:</p> <ul> <li>Development: the branch that fixes or features are continually merged.</li> <li>Staging: where development branch fixes/features are grouped and tested   together, with the intention to push to production once QA/QC passes.</li> <li>Production: where staging is stabilised and released at intervals as actual   versions of your tool/software.</li> </ul> <p>Additional supporting branches:</p> <ul> <li>Fix: fixes a bug or issue.</li> <li>Feature: add a new feature that didn't exist before.</li> <li>Hotfix: if an issue is found after a production release is made, a hotfix   can be used to patch the production code.</li> </ul>"},{"location":"dev-guide/git/#git-flow","title":"Git Flow","text":"<p>Git flow is a branching model.</p> <p>The most basic version of this would be:</p> <p><code>feature</code> or <code>fix</code> --&gt; <code>main</code> (production)</p> <p></p> <p>Adding in the extra steps of the described development lifecycle, we get:</p> <p><code>feature</code> or <code>fix</code> --&gt; <code>development</code> --&gt; <code>staging</code> --&gt; <code>main</code> (production)</p> <p></p> <p>The number of additional stages can be flexible depending on requirements.</p>"},{"location":"dev-guide/git/#forking","title":"Forking","text":"<p>To work on an open-source repository, generally you may not have direct access to the repo from the start.</p> <p>A common pattern is to make your own copy of the repo, a 'fork' of it to work on.</p> <p>Within this repo you create a new branch:</p> <pre><code>git checkout -b feat/some-new-feature\n</code></pre> <p>Then when you push the branch to your fork, generally a code hosting platform like Github/Gitlab will prompt you to create a Pull Request or Merge Request (the same thing).</p>"},{"location":"dev-guide/git/#pull-requests","title":"Pull Requests","text":"<p>A pull requests (PR) is used to merge the code from your forked repository into the original code repository.</p> <p>You should describe as accurately as possible what solution your code provides, or feature it adds, and why it is necessary.</p> <p>Ideally try to link it to an existing Issue in the repository issue board.</p> <p>The maintainer of the repo will review your code, comment, and merge it in.</p>"},{"location":"dev-guide/git/#rebasing","title":"Rebasing","text":"<p>This is often a scary concept to many.</p> <p>It essentially re-writes the Git history on a branch, so use with care.</p> <p>Use case: sometimes your code gets out of sync with the target branch you originally branched from.</p> <p>For example you branched from <code>develop</code> to a branch <code>feat/some-new-thing</code>.</p> <p>If you wish to pull in the latest updates from the <code>develop</code> branch into your feature branch, you can do a rebase:</p> <pre><code>git checkout develop\ngit pull\ngit checkout feat/some-new-thing\ngit rebase develop\n</code></pre> <p>This will insert the updates below your code edits. I.e. the history will show your commits on top of the most recent <code>develop</code> commits.</p> <p>Visually this will be:</p> <p></p>"},{"location":"dev-guide/git/#merge-vs-rebase","title":"Merge vs Rebase","text":"<ul> <li>Use merge for feature \u2192 main (work finished).<ul> <li>When on develop use merge to include a (idealy finished) feature</li> </ul> </li> <li>Use rebase for develop \u2192 feature (work in progress).<ul> <li>When on the feature branch use rebase from develop to include   the latest changes</li> <li>Regularly rebasing feature branches will keep them up to   date with current features (either from main or develop).</li> <li>This means that conflicts can be resolved gradually,   instead of in one go during a merge.</li> </ul> </li> </ul> <p>The Perils of Rebase</p> <p>The most important take away: rebase is a powerful tool, but be wary using it if you are collaborating with someone on the same feature branch.</p> <p>If a teammate happens to rebase a branch that you are working on, the easiest solution is to stash and reset to get the rebased edits:</p> <pre><code>git stash -u\ngit fetch origin feat/new-thing\ngit reset --hard feat/new-thing\ngit stash apply\ngit stash drop\n</code></pre>"},{"location":"dev-guide/git/#anticipating-a-merge","title":"Anticipating a Merge","text":"<ul> <li>Often in a developers workflow, they create on PR, then while waiting for review   work on another PR.</li> <li>If <code>PR-2</code> relies on work from <code>PR-1</code> to be merged, this can be an issue.</li> <li>One approach to solving this dilemma is informally called 'anticipating a merge'.</li> </ul>"},{"location":"dev-guide/git/#anticipating-a-merge-workflow","title":"Anticipating a Merge Workflow","text":"<ol> <li> <p>Complete work on branch <code>PR-1</code> and push to create a PR.</p> </li> <li> <p>Ensure you are on branch <code>PR-1</code>:</p> <pre><code>git checkout pr-1\n</code></pre> </li> <li> <p>Create branch <code>PR-2</code> based off <code>PR-1</code>:</p> <pre><code>git checkout -b pr-2\n</code></pre> </li> <li> <p>Build your feature on top of the code in <code>PR-1</code>.</p> </li> <li>Complete work on branch <code>PR-2</code> and push to create a PR.</li> </ol> <p>Important Notes:</p> <ul> <li>The new <code>PR-2</code> will initially include the commits from <code>PR-1</code>.</li> <li>However, the commits shown in the PR changelog will disappear once <code>PR-1</code> is merged   into the target branch (e.g. <code>development</code>).</li> <li>You should mention in the <code>PR-2</code> description that this PR 'relies on PR-1 being   accepted and merged'.</li> </ul>"},{"location":"dev-guide/git/#updating-pr-1-while-pr-2-is-in-progress","title":"Updating PR-1 While PR-2 Is In Progress","text":"<ul> <li>You may encounter a situation where the review of <code>PR-1</code> takes some time.</li> <li>If <code>PR-2</code> makes progress, but the reviewer says that <code>PR-1</code> requires updates.</li> <li>In this case you have two options:<ul> <li>Merge <code>PR-1</code> and add the required updates to <code>PR-2</code>.</li> <li>Update <code>PR-1</code> and merge, then rebase <code>PR-2</code> against the target branch.</li> </ul> </li> </ul>"},{"location":"dev-guide/intro/","title":"Development Guide","text":"<p>The HOT Tech Team comprises a dynamic assembly of highly skilled individuals, each bringing a diverse range of expertise and backgrounds. United by a shared commitment, our goal is clear: leveraging technology to contribute to a better world.</p> <p>The tech team includes a core of software developers who co-develop many of the tools listed in this documentation index.</p> <p>There is also a large community of developers who helps to facilitate this work: from consultants, NGOs, and innovative tech agencies distributed throughout the world.</p> <p>The aim of this development guide is twofold:</p>"},{"location":"dev-guide/intro/#guidelines-for-collaborators","title":"Guidelines for Collaborators","text":"<p>Open-source is at the heart of everything we do. In order to collaborate effectively, it is important to have a set of agreed upon standards and practices that developers must loosely follow.</p>"},{"location":"dev-guide/intro/#a-resource-for-new-developers","title":"A Resource for New Developers","text":"<p>We encourage new developers to start their open-source contribution journey with us. We are a welcoming, considerate, and patient bunch.</p> <p>The consensus-based information here is not an authoritative source of the only approach to software development. Nevertheless, we firmly believe that adhering to the recommendations in this guide will undoubtedly set you on the path to becoming a skilled software professional, grounded in our collective experiences at HOT.</p> <p>If these documents help a single new developer on their journey from coding zero to hero, then it has been a success.</p>"},{"location":"dev-guide/kubernetes-local-kind/","title":"Testing with Kubernetes Locally","text":"<p>We will use an official tool Kubernetes-In-Docker (KIND) to run Kubernetes via Docker locally.</p> <p>This is useful for testing Kubernetes configuration locally, and trialing software too.</p>"},{"location":"dev-guide/kubernetes-local-kind/#install-tools","title":"Install Tools","text":""},{"location":"dev-guide/kubernetes-local-kind/#kubectl","title":"Kubectl","text":"<p>Used to control Kubernetes clusters.</p> <pre><code>temp_dir=$(mktemp -d)\ncd \"${temp_dir}\"\n\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kubectl \\\nhttps://dl.k8s.io/release/\\\n$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/\\\n$(uname -s | tr '[:upper:]' '[:lower:]')/amd64/kubectl\n\nchmod +x ./kubectl\nsudo mv ./kubectl /usr/local/bin/kubectl\n\nrm -rf \"$temp_dir\"\n</code></pre>"},{"location":"dev-guide/kubernetes-local-kind/#kind","title":"KIND","text":"<p>Used to run a Kubernetes cluster locally.</p> <pre><code>temp_dir=$(mktemp -d)\ncd \"${temp_dir}\"\n\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind \\\nhttps://kind.sigs.k8s.io/dl/v0.22.0/kind-$(uname)-amd64\n\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n\nrm -rf \"$temp_dir\"\n</code></pre> Optional Config   Kubie: used to easily switch Kubernetes context (i.e. multiple clusters).  <pre><code>temp_dir=$(mktemp -d)\ncd \"${temp_dir}\"\n\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kubie \\\nhttps://github.com/sbstp/kubie/releases/download/v0.23.0/kubie-\\\n$(uname -s | tr '[:upper:]' '[:lower:]')-amd64\n\nchmod +x ./kubie\nsudo mv ./kubie /usr/local/bin/kubie\n\nrm -rf \"$temp_dir\"\n</code></pre>  Helm: used to install software into the cluster.  <pre><code>temp_dir=$(mktemp -d)\ncd \"${temp_dir}\"\n\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./helm.tar.gz \\\nhttps://get.helm.sh/helm-v3.14.3-$(uname -s | tr '[:upper:]' '[:lower:]')-amd64.tar.gz\n\ntar -xvzf helm.tar.gz\nsudo mv $(uname -s | tr '[:upper:]' '[:lower:]')-amd64/helm /usr/local/bin/helm\n\nrm -rf \"$temp_dir\"\n</code></pre>  BASH (bashrc) aliases  <pre><code>echo alias k='kubectl' &gt;&gt; ~/.bashrc\necho alias kcc='kubie ctx' &gt;&gt; ~/.bashrc\necho alias ns='kubie ns' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>  fish aliases  <pre><code>echo alias k='kubectl' &gt;&gt; ~/.config/fish/config.fish\necho alias kcc='kubie ctx' &gt;&gt; ~/.config/fish/config.fish\necho alias ns='kubie ns' &gt;&gt; ~/.config/fish/config.fish\nsource ~/.config/fish/config.fish\n</code></pre>"},{"location":"dev-guide/kubernetes-local-kind/#create-a-cluster","title":"Create a Cluster","text":"<ul> <li>Run the following to create a cluster with Ingress ports bound:</li> </ul> <pre><code>cat &lt;&lt;EOF | kind create cluster --name local --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 7080\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 7433\n    protocol: TCP\nEOF\n</code></pre> <p>The cluster will be named 'kind-local'</p> Fish shell equivalent <pre><code>kind create cluster --name local --config (echo '\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 7080\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 7433\n    protocol: TCP\n' | psub)\n</code></pre> <p>Cluster services will be accessible under http://localhost:7080</p> <p>Change the hostPort variable if you wish to use a different port.</p>"},{"location":"dev-guide/kubernetes-local-kind/#deploy-the-ingress-controller","title":"Deploy the Ingress Controller","text":"<p>Contour uses Envoy and may be a good choice in production.</p> <p>But the simplest and most battle tested in the Nginx ingress.</p> <p>Deploy with:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\n</code></pre> <p>Check when it is ready:</p> <pre><code>kubectl wait --namespace ingress-nginx \\\n  --for=condition=ready pod \\\n  --selector=app.kubernetes.io/component=controller \\\n  --timeout=90s\n</code></pre>"},{"location":"dev-guide/kubernetes-local-kind/#test-the-ingress","title":"Test the Ingress","text":"<p>Run test service:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply --filename=-\nkind: Pod\napiVersion: v1\nmetadata:\n  name: foo-app\n  labels:\n    app: foo\nspec:\n  containers:\n  - command:\n    - /agnhost\n    - netexec\n    - --http-port\n    - \"8080\"\n    image: registry.k8s.io/e2e-test-images/agnhost:2.39\n    name: foo-app\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: foo-service\nspec:\n  selector:\n    app: foo\n  ports:\n  # Default port used by the image\n  - port: 8080\n---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: bar-app\n  labels:\n    app: bar\nspec:\n  containers:\n  - command:\n    - /agnhost\n    - netexec\n    - --http-port\n    - \"8080\"\n    image: registry.k8s.io/e2e-test-images/agnhost:2.39\n    name: bar-app\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: bar-service\nspec:\n  selector:\n    app: bar\n  ports:\n  # Default port used by the image\n  - port: 8080\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  rules:\n  - http:\n      paths:\n      - pathType: Prefix\n        path: /foo(/|$)(.*)\n        backend:\n          service:\n            name: foo-service\n            port:\n              number: 8080\n      - pathType: Prefix\n        path: /bar(/|$)(.*)\n        backend:\n          service:\n            name: bar-service\n            port:\n              number: 8080\n---\nEOF\n</code></pre> Fish shell equivalent <pre><code>kubectl apply --filename (echo '\nkind: Pod\napiVersion: v1\nmetadata:\n  name: foo-app\n  labels:\n    app: foo\nspec:\n  containers:\n  - command:\n    - /agnhost\n    - netexec\n    - --http-port\n    - \"8080\"\n    image: registry.k8s.io/e2e-test-images/agnhost:2.39\n    name: foo-app\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: foo-service\nspec:\n  selector:\n    app: foo\n  ports:\n  # Default port used by the image\n  - port: 8080\n---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: bar-app\n  labels:\n    app: bar\nspec:\n  containers:\n  - command:\n    - /agnhost\n    - netexec\n    - --http-port\n    - \"8080\"\n    image: registry.k8s.io/e2e-test-images/agnhost:2.39\n    name: bar-app\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: bar-service\nspec:\n  selector:\n    app: bar\n  ports:\n  # Default port used by the image\n  - port: 8080\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  rules:\n  - http:\n      paths:\n      - pathType: Prefix\n        path: /foo(/|$)(.*)\n        backend:\n          service:\n            name: foo-service\n            port:\n              number: 8080\n      - pathType: Prefix\n        path: /bar(/|$)(.*)\n        backend:\n          service:\n            name: bar-service\n            port:\n              number: 8080\n---\n' | psub)\n</code></pre> <p>$ curl localhost:7080/foo/hostname</p> <p>should output \"foo-app\"</p> <p>$ curl localhost:7080/bar/hostname</p> <p>should output \"bar-app\"</p> <p>Cleanup test resources:</p> <pre><code>kubectl delete pod foo-app\nkubectl delete pod bar-app\nkubectl delete svc foo-service\nkubectl delete svc bar-service\nkubectl delete ingress example-ingress\n</code></pre>"},{"location":"dev-guide/licensing/","title":"Licensing","text":"<p>With so many options for licensing code, the topic can be quite confusing. Many projects like Boost, Eclipse, Apache, etc.. have their own license, often based on the GPL, which only apply to that project, so aren't appropriate for use at HOT. There are two primary types of licenses, permissive and non-permissive. Non permissive licenses may appear to be open source, but often it's a few clauses in the licenses that make them unable to be used in other open source projects.</p> <p>HOT believes strongly in the freedoms embodied on the free software culture, while following the collaborative development processes common for open source software projects.</p> <p>All of the software at HOT uses permissive licenses, commonly the GPLv3 for compiled code, the AGPLv3 for code shared over a network (a website), and some older code using the BSD license or pubic domain.</p>"},{"location":"dev-guide/licensing/#the-gplv3-and-agplv3","title":"The GPLv3 and AGPLv3","text":"<ul> <li>These are strong 'copyleft' licences, meaning that anyone modifiying the software is required to make those changes openly available. This is commonly called the derivative work clause. Any derivative software must be contributes it back to the original software project with no restrictions.</li> </ul> <ul> <li>The main purpose of this license is to prevent commercial   exploitation of open code, making any modifications open so the   entire community can benefit.</li> </ul>"},{"location":"dev-guide/licensing/#other-content-is-cc-by","title":"Other Content is CC-BY","text":"<ul> <li>Creative content, such as translations and designs are copyrighted   different.</li> <li>As with AGPL for code, this license is 'copyleft'</li> <li>It allows others modify and built upon our own, even for commercial   purposes, as long as they credit us and   license their work under the same terms as CC-BY.</li> </ul>"},{"location":"dev-guide/licensing/#what-licenses-can-we-use","title":"What Licenses Can We Use","text":"<ul> <li>We use many underlying packages and tools within our software.</li> <li>Any contributor must be aware of licensing before adding any   additional external code to our software.</li> </ul>"},{"location":"dev-guide/licensing/#compatible-licenses","title":"Compatible Licenses","text":"<p>This is a short list of copyleft compatible licenses. For more detail, refer to the Free Software Foundation (FSF) page on compatible and incompatible licenses. This in far from an exhaustive list, only the most common ones are listed here.</p> <ul> <li>Public Domain</li> <li>Mozilla Public License (MPL) version 2.0</li> <li>Apache License 2.0</li> <li>Modified BSD license</li> <li>Intel Open Source License</li> <li>FreeBSD license</li> </ul>"},{"location":"dev-guide/licensing/#incompatible-licenses","title":"Incompatible Licenses","text":"<p>The Open Source Initiative also has a list of approved open source licenses. Not all of these are compatible with copylefted software, so not appropriate for code at HOT even though they are considered premissive licenses.</p> <p>Here is a short list of common non-permissive licenses, and can't be used for any code in HOT software projects.</p> <ul> <li>Mozilla Public License (MPL) version 1.1</li> <li>Apache License, Version 1.0 and 1.1</li> <li>Apple Public Source License (APSL), version 2</li> <li>Microsoft Public License (Ms-PL)</li> <li>NASA Open Source Agreement</li> <li>Original BSD license</li> <li>Common Development and Distribution License (CDDL), version 1.0</li> <li>Common Public Attribution License 1.0 (CPAL)</li> <li>Common Public License Version 1.0</li> <li>European Union Public License (EUPL) version 1.1 and 1.2</li> <li>IBM Public License, Version 1.0</li> <li>Open Software License, all versions through 3.0</li> </ul>"},{"location":"dev-guide/pre-commit/","title":"Pre-Commit Hooks","text":""},{"location":"dev-guide/pre-commit/#git-hooks","title":"Git Hooks","text":"<p>Git has hooks, which can run code on various events:</p> <ul> <li>Prior to commiting code (pre-commit).</li> <li>After committing code (post-commit).</li> <li>After checking out a branch (post-checkout).</li> <li>Before rebasing (pre-rebase).</li> <li>Etc.</li> </ul>"},{"location":"dev-guide/pre-commit/#pre-commit-hook","title":"Pre-Commit Hook","text":"<ul> <li>Pre-commit Git hooks run checks before a commit is accepted.</li> <li>Pre-commit is a package to automate the setup.</li> <li>After config, issues such as syntax errors and security flaws are picked up.</li> <li>Used mostly for formatting and linting.</li> <li>Linting is checking code against a set of formatting rules   and for syntax errors.</li> </ul>"},{"location":"dev-guide/pre-commit/#pre-commit-python-tool","title":"Pre-Commit (Python Tool)","text":"<ul> <li>Pre-commit is a Python tool for simplifying   and applying Git pre-commit hooks.</li> <li>Hooks can be configured via a YAML file, then applied on each attempted commit.</li> <li>There are many hooks available from different sources.</li> </ul> <p>Install it with:</p> <pre><code>pip install pre-commit\n</code></pre>"},{"location":"dev-guide/pre-commit/#add-pre-commit-configyaml","title":"Add pre-commit-config.yaml","text":"<ul> <li>Add a <code>pre-commit-config.yaml</code> to your repo root.</li> <li>A best practice config file, taken from FMTM:</li> </ul> <pre><code>repos:\n  # Versioning: Commit messages &amp; changelog\n  - repo: https://github.com/commitizen-tools/commitizen\n    rev: v3.13.0\n    hooks:\n      - id: commitizen\n        stages: [commit-msg]\n\n  # Lint / autoformat: Python code\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    # Ruff version.\n    rev: \"v0.1.13\"\n    hooks:\n      # Run the linter\n      - id: ruff\n        args: [--fix, --exit-non-zero-on-fix]\n      # Run the formatter\n      - id: ruff-format\n\n  # Autoformat: YAML, JSON, Markdown, etc.\n  - repo: https://github.com/pre-commit/mirrors-prettier\n    rev: v3.1.0\n    hooks:\n      - id: prettier\n        args:\n          [\n            --ignore-unknown,\n            --no-error-on-unmatched-pattern,\n            \"!CHANGELOG.md\",\n            \"!CONTRIBUTING.md\",\n            \"!src/frontend/pnpm-lock.yaml\",\n          ]\n\n  # Lint: Bash scripts\n  - repo: https://github.com/openstack-dev/bashate.git\n    rev: 2.1.1\n    hooks:\n      - id: bashate\n\n  # Lint: Shell scripts\n  - repo: https://github.com/shellcheck-py/shellcheck-py\n    rev: v0.9.0.6\n    hooks:\n      - id: shellcheck\n        args: [\"-x\"]\n\n  # Lint: Markdown\n  - repo: https://github.com/igorshubovych/markdownlint-cli\n    rev: v0.38.0\n    hooks:\n      - id: markdownlint\n        args: [--fix, --ignore, CHANGELOG.md, --ignore, .github]\n</code></pre> <p>Note: the config above is for a monorepo configuration.</p> <p>Your repo may not require both Python and JS code formatting.</p>"},{"location":"dev-guide/pre-commit/#add-hooks","title":"Add Hooks","text":"<p>Run</p> <pre><code># Standard install for most hooks\npre-commit install\n\n# Additional commit-msg hook (for the commitizen hook above)\npre-commit install --hook-type commit-msg\n</code></pre> <p>Now when you attempt to commit to the repo:</p> <ul> <li>Code will be auto-formatted.</li> <li>An error will show if linting fails.</li> <li>An error will show if commit messages are in the wrong format.</li> </ul>"},{"location":"dev-guide/testing/","title":"Testing","text":""},{"location":"dev-guide/testing/#types-of-testing","title":"Types of Testing","text":""},{"location":"dev-guide/testing/#unit-testing","title":"Unit Testing","text":"<ul> <li>Testing specific isolated functions or classes in your code.</li> <li>Inputs are pre-defined and outputs are mocked to verify the code   works as intended.</li> <li>Unit tests can be written for the backend (e.g. Pytest) and frontend   (e.g. Vitest).</li> </ul>"},{"location":"dev-guide/testing/#integration-testing","title":"Integration Testing","text":"<ul> <li>Verifies that different units or modules work together as expected.</li> <li>For example and endpoint on a web API can be tested as such:<ul> <li>Call and endpoint that triggers code in various modules.</li> <li>Standarise the input to the test, for example a specific JSON.</li> <li>Check the output data matches the expected output, every time it runs.</li> </ul> </li> <li>Typically backend tests, but can also be written to group together   multiple frontend unit tests (for example, testing a component state).</li> </ul>"},{"location":"dev-guide/testing/#end-to-end-e2e-testing","title":"End-to-End (E2E) Testing","text":"<ul> <li>Similar to integration tests, except the entire expected user   workflow is tested (i.e. the ordering is important).</li> <li>For example, if you expect a user to create a user account --&gt;   create a project --&gt; perform some processing, then these steps will   be tested in order (as if the user was using the software).</li> </ul>"},{"location":"dev-guide/testing/#user-interface-ui-testing","title":"User Interface (UI) Testing","text":"<ul> <li>Checking for changes to the user interface functionality can be   automated.</li> <li>As of 2024, a great tool to do this is   Playwright.</li> <li>The idea is to navigate to various pages and see if the output is as   expected.</li> <li>For example, a screenshot of a page can be taken on a page load, then   compared against a page load in a future test. This will determine if   any unexpected visual regression has occured.</li> <li>Page elements can be navigated and interacted with using various   properties, such as element   names, ids, labels, to lower level CSS and XPath if required.</li> </ul>"},{"location":"dev-guide/testing/#performance-testing","title":"Performance Testing","text":"<ul> <li>Evaluates the system's responsiveness, scalability, and stability   under different conditions.</li> <li>Includes:<ul> <li>Load testing, for expected and peak load.</li> <li>Stress testing, for abnormally high load / upper limit testing.</li> <li>Scalability testing, used to identify optimal configuration to handle load.</li> </ul> </li> <li>Generally for backend or database code.</li> <li>An example could be using a profiler to determine response time in a web API,   or using <code>EXPLAIN ANALYSE</code> on a database to determine bottlenecks.</li> </ul>"},{"location":"dev-guide/testing/#security-testing","title":"Security Testing","text":"<ul> <li>Identifies Common Vulnerabilities and Exposures (CVEs) in software.</li> <li>CVEs are publically available lists of identified security flaws in code.</li> <li>One way to test this could be container image scanning, which checks for   vulnerabilities in the underlying operating system, plus the code and dependencies.</li> </ul>"},{"location":"dev-guide/testing/#smoke-testing","title":"Smoke Testing","text":"<ul> <li>A test to pretty much see if your application starts up.</li> <li>If a smoke test fails, then you application failed to initialise.</li> <li>Particularly useful to test if a container runs as expected.</li> <li>Useful to include in CI to prevent deployment if smoke test fails.</li> </ul>"},{"location":"dev-guide/testing/#testing-terminology","title":"Testing Terminology","text":""},{"location":"dev-guide/testing/#white-box-testing","title":"White Box Testing","text":"<ul> <li>Examines the internal logic, code structure, and implementation   details of the software.</li> <li>Requires knowledge of the internal workings of the application.</li> </ul>"},{"location":"dev-guide/testing/#black-box-testing","title":"Black Box Testing","text":"<ul> <li>Tests the software's functionality without knowing its internal code or logic.</li> <li>Focuses on inputs and outputs, treating the software as a \"black box\".</li> </ul>"},{"location":"dev-guide/testing/#alpha-testing","title":"Alpha Testing","text":"<ul> <li>Conducted by a select group of users before the software's public release.</li> <li>Focuses on identifying major issues before wider deployment.</li> </ul>"},{"location":"dev-guide/testing/#beta-testing","title":"Beta Testing","text":"<ul> <li>Conducted by a larger group of users in a real-world environment.</li> <li>Gathers feedback from end-users to improve the software before the final release.</li> </ul>"},{"location":"dev-guide/testing/#python","title":"Python","text":""},{"location":"dev-guide/testing/#pytest","title":"PyTest","text":"<ul> <li>PyTest is a popular unit testing framework for Python, with more   in-built functionality than the standard Python <code>unittest</code> module.</li> <li>It has a simple and concise syntax, many plugins, plus async support,   and detailed / informative reporting.</li> <li>Two major advantages are:<ul> <li>Fixtures: allows you to set up and tear down resources during   testing, for example database entries.</li> <li>Parameterised testing: allowing you to run the same test with   different input values.</li> </ul> </li> <li>With the help of plugins, it's easy to write integration tests too.</li> <li>For example a FastAPI endpoint can be called with inputs, to test the   outputs are as expected.</li> </ul> <p>Usage:</p> <pre><code>pytest\n</code></pre>"},{"location":"dev-guide/testing/#coverage","title":"Coverage","text":"<ul> <li>The concept of coverage is to determine the amount of your code that   is executed during unit tests.</li> <li>This metric is a rough approximation of how thoroughly your tests   actually test your code.</li> <li>Use with care, this is only an assessment of how much code is touched by   your tests. A 100% coverage codebase may still be poorly tested.</li> </ul> <p>Usage:</p> <pre><code>coverage run -m pytest\n</code></pre> <p>Generating reports:</p> <pre><code>coverage report -m\n</code></pre>"},{"location":"dev-guide/testing/#javascript","title":"Javascript","text":""},{"location":"dev-guide/testing/#vitest","title":"ViTest","text":""},{"location":"dev-guide/testing/#playwright","title":"Playwright","text":""},{"location":"dev-guide/version-control/","title":"Version Control","text":""},{"location":"dev-guide/version-control/#semantic-versioning-semver","title":"Semantic Versioning (SemVer)","text":"<ul> <li>A method of versioning your software packages.</li> <li>Makes most sense for versioning software that is used by another piece   of software, e.g. a Python package that is installed.</li> <li>The version number is composed of three segments: <code>MAJOR.MINOR.PATCH</code>,   for example <code>1.4.2</code>.<ul> <li>MAJOR: Increments when incompatible API changes are introduced.   It indicates that there are significant and potentially breaking   changes that might require modifications in existing code.</li> <li>MINOR: Increments when new features are added in a backward-compatible manner.   It signals the addition of functionality without breaking existing code.   Developers can expect that their code will still work as intended.</li> <li>PATCH: Increments for backward-compatible bug fixes.   It denotes minor improvements or bug fixes that do not introduce new   features and do not break existing functionality.</li> </ul> </li> </ul>"},{"location":"dev-guide/version-control/#calver-and-others","title":"CalVer and others","text":"<ul> <li>Sometimes SemVer doesn't make sense. For example, in a user-facing   software service/tool, what does a 'breaking' change denote?</li> <li>In these cases, other conventions may be better suited.</li> <li>At HOT, we use <code>YYYY.VERSION.PATCH</code>, in line with the developers of the ODK ecosystem.<ul> <li>YYYY: the current year.</li> <li>VERSION: the release version for the current year. Resets in the next year.</li> <li>PATCH: a patch to the released version, typically bugfixes.</li> </ul> </li> </ul>"},{"location":"dev-guide/version-control/#conventional-commits","title":"Conventional Commits","text":"<p>A specification for adding human and machine readable meaning to commit messages.</p> <p>A 'new' standard for commit messages.</p> <p>Format: <code>&lt;type&gt;[optional scope]: &lt;description&gt;</code></p> <p>Example <code>feat: allow provided config object to extend other configs</code> Example <code>fix: fixed the bug in issue #123</code></p> <p>Advantage: Automated SemVer version management (major.minor.patch), and automated changelogs.</p>"},{"location":"dev-guide/version-control/#tool-commitizen","title":"Tool: Commitizen","text":"<ul> <li>Commitizen is a   Python tool to help with creating conventional commits and   automating version control.</li> </ul> <ul> <li>Versions are managed by Commitizen from the <code>pyproject.toml</code> file in a   repo.</li> </ul> <ul> <li>Versions are determined by conventional commit messages:<ul> <li><code>fix: xxx</code> denotes a patch, <code>feat: xxx</code> denotes a minor increment.</li> <li>Breaking changes are applied every year to increment the <code>YYYY</code> in   place of <code>MAJOR</code>.</li> </ul> </li> </ul>"},{"location":"dev-guide/version-control/#install","title":"Install","text":"<p><code>pip install commitizen</code></p>"},{"location":"dev-guide/version-control/#commiting-code","title":"Commiting Code","text":"<ul> <li>Instead of <code>git commit</code> use <code>cz commit</code> and follow the prompts.</li> <li>You can select the type of commit, plus additional metadata.</li> </ul>"},{"location":"dev-guide/version-control/#bumping-a-version","title":"Bumping a Version","text":"<ul> <li>When you decide it is time to create a new version:</li> </ul> <ol> <li> <p>Create a new branch</p> <p><code>git checkout -b bump/new_release</code></p> </li> <li> <p>Bump the version and push</p> <pre><code>pip install commitizen # (if not installed)\n\ncz bump --check-consistency --changelog\n\ngit push\ngit push --tag\n</code></pre> </li> </ol> <p>This will:</p> <ul> <li>Update the SemVer version number in locations specific in <code>pyproject.toml</code>,   throughout the codebase.<ul> <li>If a <code>feat</code> commit is included, the version is bumped by a minor   increment (0.x.0), if only <code>fix</code> is included a patch will be used   (0.0.x).</li> </ul> </li> <li>Automatically update CHANGELOG.md with all changes since the last version.</li> <li>Create a tag matching the version number.</li> </ul> <p>Note: in a repo where you have direct push access, you would simply update on main and push. As we are using Git-Flow, a PR is necessary.</p>"},{"location":"dev-guide/version-control/#creating-releases","title":"Creating Releases","text":"<ol> <li>Update the version throughout the code (Bumping a Version).</li> <li>Go to the Releases page of your repo    (https://github.com/ORG/REPO/releases).</li> <li>Click <code>Draft a new release</code>.</li> <li>Click <code>Choose a tag</code>, then input the current version number and press    enter (this will automatically create a matching tag for your release).</li> <li>Set the <code>Release title</code> to v<code>x.x.x</code>, replacing with your version number.</li> <li>Add a description if possible, then release.</li> </ol> <p>This should trigger the PyPi publishing workflow (for HOT repos), and your version will be available on PyPi.org.</p>"},{"location":"dev-guide/web-apis/","title":"Web APIs","text":""},{"location":"dev-guide/web-apis/#types","title":"Types","text":"<p>As a small aside, REST is not the only standard available when it comes to web APIs.</p>"},{"location":"dev-guide/web-apis/#rest","title":"REST","text":"<p>REST has dominated the scene for quite a few years.</p> <p>URLs are mapped to different HTTP methods (GET, POST, PUT, DELETE) to perform an action when called.</p> <p>Responses can be divided between Data APIs (return JSON) vs Hypermedia APIs (return HTML).</p>"},{"location":"dev-guide/web-apis/#graphql","title":"GraphQL","text":"<p>Without going into the details, this standard has many advantages over REST Data APIs, with much more efficient queries being possible.</p>"},{"location":"dev-guide/web-apis/#rpc","title":"RPC","text":"<p>The Remote Procedure Call (RPC) protocol can return XML or JSON responses.</p> <p>It is used to trigger code remotely, so good for internal communication between different services. HOT uses gRPC for internal communication between tools, outside of their REST API.</p> <p>While a RESTful API returns a document, the response from an RPC server is confirmation that the function was triggered, or an error indicating why it failed to run.</p>"},{"location":"dev-guide/web-apis/#others","title":"Others","text":"<p>SOAP is a historic API design using XML, and is no longer recommended.</p>"},{"location":"dev-guide/web-apis/#what-to-choose","title":"What To Choose","text":"<p>As of 2023, Data APIs have been key for the adoption of Single Page Applications (SPA) and Javascript frameworks (where JSON data is manipulated by the frontend).</p> <p>Going forward, Hypermedia APIs are re-emerging as an increasingly important alternative, where the entire page is rendered before being returned (reducing the need for things like Server Side Rendering (SSR)).</p> <p>The HTMX website has many interesting essays on this topic.</p> <p>In summary, it is probably best to default to a Hypermedia REST API, with a simple web framework like HTMX. If a much more complex frontend is required (such as a word processor, graphics editor, complex map), then a Data REST API is the best option.</p>"},{"location":"dev-guide/web-apis/#frameworks","title":"Frameworks","text":"<p>API Frameworks are generally divided into synchronous and asynchronous.</p> <p>Async is a newer paradigm in Python, often slightly more complex to code, but should be faster and more suited to a web API.</p> <p>Synchronous frameworks include flask, Django, etc.</p> <p>The asynchronous framework we recommend at HOT, as of 2024, is FastAPI. It's what we use for most projects.</p> <p>There is a great comparison with other frameworks in the ecosystem available.</p> <p>Another contender would be LiteStar, a project spawned from some frustrations with the governance of FastAPI.</p>"},{"location":"dev-guide/web-apis/#fastapi","title":"FastAPI","text":"<p>These docs provide some helpful info for FastAPI best practices.</p>"},{"location":"dev-guide/web-apis/#async-programming","title":"Async Programming","text":"<p>Asynchronous programming can be a learning curve for Python developers.</p> <ul> <li>FastAPI is an asynchronous web framework that is built to use async code.</li> <li>Using async (<code>async def</code>) function with await is more scalable than   using synchronous code <code>def</code>, so this is always the preferred default   approach.</li> <li>Using synchronous code is possible, but devs should be aware of the pitfalls:   if the code runs for a long time, it will block the async event loop   (i.e. block the thread until the process completes).<ul> <li>Bear in mind that 'synchronous' code could be from what you write   in the crud functions, OR could be from a library that you use   (e.g. osm-fieldwork is synchronous for the most part).</li> </ul> </li> </ul>"},{"location":"dev-guide/web-apis/#workers-thread-blocking","title":"Workers &amp; Thread Blocking","text":"<ul> <li>We run FastAPI (uvicorn) with a number of workers defined. This is the   number of threads available to run processes.</li> <li>If a process blocks a thread (as described above), then the remaining threads   are available to take new requests.</li> <li>If all of the workers/threads are blocked by tasks, the server will hang / be unresponsive!</li> </ul>"},{"location":"dev-guide/web-apis/#using-synchronous-code","title":"Using Synchronous Code","text":"<p>It is of course possible to use synchronous code, but if necessary, be sure to run this in another thread.</p> <p>To do this you have several options.</p>"},{"location":"dev-guide/web-apis/#options","title":"Options","text":""},{"location":"dev-guide/web-apis/#1-using-sync-code-within-an-async-def-function","title":"1) Using sync code within an <code>async def</code> function","text":"<ul> <li>Use FastAPI BackgroundTasks, with polling for the task completion.<ul> <li>The task should be written as a standard <code>def</code>. FastAPI will handle   this automatically and ensure it runs in a separate thread.</li> </ul> </li> <li>Alternatively, if you wish to run the task in the foreground and return   the response, use the FastAPI helper <code>run_in_threadpool</code>.<ul> <li>This will run the function in a separate thread to ensure that the main   thread does not get blocked.</li> </ul> </li> </ul> <pre><code>from fastapi.concurrency import run_in_threadpool\n\ndef long_running_sync_task(time_to_sleep):\n    sleep(time_to_sleep)\n\nasync def some_func():\n    data = await run_in_threadpool(lambda: long_running_sync_task(10))\n</code></pre>"},{"location":"dev-guide/web-apis/#2-running-multiple-standard-def-from-within-an-async-def-function","title":"2) Running multiple standard <code>def</code> from within an <code>async def</code> function","text":"<ul> <li>Sometimes you need to run multiple <code>def</code> functions in parallel.</li> <li>To do this, you can use ThreadPoolExecutor:</li> </ul> <pre><code>from concurrent.futures import ThreadPoolExecutor, wait\n\ndef a_synchronous_function(db):\n    # Run with expensive task via threadpool\n    def wrap_generate_task_files(task):\n        \"\"\"Func to wrap and return errors from thread.\n\n        Also passes it's own database session for thread safety.\n        If we pass a single db session to multiple threads,\n        there may be inconsistencies or errors.\n        \"\"\"\n        try:\n            generate_task_files(\n                next(get_db()),\n                project_id,\n                task,\n                xlsform,\n                form_type,\n                odk_credentials,\n            )\n        except Exception as e:\n            log.exception(str(e))\n\n    # Use a ThreadPoolExecutor to run the synchronous code in threads\n    with ThreadPoolExecutor() as executor:\n        # Submit tasks to the thread pool\n        futures = [\n            executor.submit(wrap_generate_task_files, task)\n            for task in tasks_list\n        ]\n        # Wait for all tasks to complete\n        wait(futures)\n</code></pre> <p>Note that in the above example, we cannot pass the db object from the parent function into the functions spawned in threads. This is becaue a single database connection should not be written to by multiple processes at the same time, as you may get data inconsistencies. To solve this we generate a new db connection within the pool for each separate task we run in a thread.</p> <p>To avoid issues, look into limiting the thread usage via: https://stackoverflow.com/questions/73195338/how-to-avoid-database-connection-pool-from-being-exhausted-when-using-fastapi-in</p>"},{"location":"dev-guide/web-apis/#3-running-an-async-def-within-a-sync-def","title":"3) Running an <code>async def</code> within a sync <code>def</code>","text":"<ul> <li>As we try to write most functions async for FastAPI, sometime we need to   run some <code>async def</code> logic within a sync <code>def</code>. This is not possible normally.</li> <li>To avoid having to write a duplicated <code>def</code> equivalent of the <code>async def</code>   code, we can use the package <code>asgiref</code>:</li> </ul> <pre><code>from asgiref.sync import async_to_sync\n\nasync def get_project(db, project_id):\n    return something\n\ndef a_sync_function():\n     get_project_sync = async_to_sync(get_project)\n     project = get_project_sync(db, project_id)\n     return project\n</code></pre>"},{"location":"dev-guide/web-apis/#4-efficiency-running-batch-async-tasks","title":"4) Efficiency running batch async tasks","text":"<ul> <li>Sometime you may have a very efficient async task you need to call   within a for loop.</li> <li>Instead of that, you can use <code>asyncio.gather</code> to much more efficiently   collect and return the async data (e.g. async web requests, or async   file requests, or async db requests):</li> </ul> <pre><code>from asyncio import gather\n\nasync def parent_func(db, project_id, data, no_of_buildings, has_data_extracts):\n    ... some other code\n\n    async def split_multi_geom_into_tasks():\n        # Use asyncio.gather to concurrently process the async generator\n        split_poly = [\n            split_polygon_into_tasks(\n                db, project_id, data, no_of_buildings, has_data_extracts\n            )\n            for data in boundary_geoms\n        ]\n\n        # Use asyncio.gather with list to collect results from the\n        # async generator\n        return (\n            item for sublist in await gather(*split_poly)\n            for item in sublist if sublist\n        )\n\n    geoms = await split_multi_geom_into_tasks()\n</code></pre>"},{"location":"dev-guide/web-apis/#note","title":"Note","text":"<ul> <li>If you regularly find you are running out of workers/threads and the   server is overloaded, it may be time to add a task queuing system to your stack.</li> <li>Celery is made for just this - defer tasks to a queue, and run gradually   to reduce the immediate load.</li> </ul>"},{"location":"dev-guide/web-apis/#best-practices","title":"Best Practices","text":""},{"location":"dev-guide/web-apis/#1-logical-project-structure","title":"1. Logical Project Structure","text":"<ul> <li>Group together related code into units.</li> <li>An example template could be:</li> </ul> <pre><code>fastapi-project\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 projects\n\u2502   \u2502   \u251c\u2500\u2500 routes.py  # endpoints + router\n\u2502   \u2502   \u251c\u2500\u2500 schemas.py  # pydantic models\n\u2502   \u2502   \u2514\u2500\u2500 logic.py  # logic separate from routes for easier testing\n\u2502   \u251c\u2500\u2500 tasks\n\u2502   \u2502   \u251c\u2500\u2500 routes.py\n\u2502   \u2502   \u251c\u2500\u2500 schemas.py\n\u2502   \u2502   \u2514\u2500\u2500 logic.py\n\u2502   \u251c\u2500\u2500 db\n\u2502   \u2502   \u251c\u2500\u2500 models.py  # global database models (can also be per subdir)\n\u2502   \u2502   \u251c\u2500\u2500 enums.py  # enum mapping for the database\n\u2502   \u2502   \u2514\u2500\u2500 database.py  # database connection config\n\u2502   \u251c\u2500\u2500 config.py  # global settings\n\u2502   \u2514\u2500\u2500 main.py\n\u251c\u2500\u2500 tests/\n</code></pre>"},{"location":"dev-guide/web-apis/#2-use-the-correct-response-type","title":"2. Use the Correct Response Type","text":"<ul> <li> <p>FastAPI has many in-built Response types:</p> <ul> <li>HTMLResponse: this would be useful paired with a HTMX frontend.</li> <li>JSONResponse: to return a JSON.</li> <li>ORJSONResponse: a faster JSON encoder. If you need to encode a large number   of object, this might be a good choice.</li> <li>RedirectResponse: use this for linking to an S3 file, to avoid handling it   in the FastAPI server (frontend goes directly to S3).</li> <li>FileResponse: load an entire file from disk and serve to the user.</li> <li>StreamingResponse: better for serving large file in chunks.</li> </ul> </li> </ul> <ul> <li> <p>Don't forget to include the correct HTTP <code>status_code</code> with your response:</p> <ul> <li>200: Success, used as the final return for most endpoints.</li> <li>204: Success, but no response data necessary.</li> <li>400: Bad request, usually malformed syntax or incorrect HTTP method (POST/GET).</li> <li>401: Unauthorized, if the client does did not provide an auth token.</li> <li>403: Forbidden, if the client does not have permission to access the content.</li> <li>404: Not found, if the requested content is not present. E.g. wrong project id.</li> <li>422: Unprocessable entity, if the request data is in the incorrect format.   e.g. a string provided in a form body variable when it should be an int.</li> <li>500: Generic error if no other error is provided, like Exception in Python.</li> </ul> </li> </ul>"},{"location":"dev-guide/web-apis/#3-use-pydantic-for-validation","title":"3. Use Pydantic for Validation","text":""},{"location":"dev-guide/web-apis/#settings-config","title":"Settings Config","text":"<pre><code>from functools import lru_cache\nfrom typing import Any, Optional\n\nfrom pydantic import PostgresDsn, ValidationInfo, field_validator\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Settings(BaseSettings):\n    \"\"\"Main settings class, defining environment variables.\"\"\"\n\n    # Required field\n    VAR1: str\n    # Required field, but nullable\n    VAR2: Optional[str]\n    # Required field, with default\n    VAR3: Optional[str] = \"7050\"\n    # Not required field\n    VAR4: Optional[str] = None\n\n    DB_HOST: Optional[str] = \"fmtm-db\"\n    DB_USER: Optional[str] = \"fmtm\"\n    DB_PASSWORD: Optional[str] = \"fmtm\"\n    DB_NAME: Optional[str] = \"fmtm\"\n\n    FMTM_DB_URL: Optional[PostgresDsn] = None\n\n    # Using a field validator to build a variable\n    @field_validator(\"FMTM_DB_URL\", mode=\"after\")\n    @classmethod\n    def assemble_db_connection(cls, v: Optional[str], info: ValidationInfo) -&gt; Any:\n        \"\"\"Build Postgres connection from environment variables.\"\"\"\n        if isinstance(v, str):\n            return v\n        pg_url = PostgresDsn.build(\n            scheme=\"postgresql\",\n            username=info.data.get(\"DB_USER\"),\n            password=info.data.get(\"DB_PASSWORD\"),\n            host=info.data.get(\"DB_HOST\"),\n            path=info.data.get(\"DB_NAME\", \"\"),\n        )\n        return pg_url\n\n    # Using env_file param loads from .env\n    model_config = SettingsConfigDict(\n        case_sensitive=True, env_file=\".env\", extra=\"allow\"\n    )\n\n# lru_cache prevents building obj every time settings.var is invoked\n@lru_cache\ndef get_settings():\n    \"\"\"Cache settings when accessed throughout app.\"\"\"\n    _settings = Settings()\n    if _settings.DEBUG:\n        print(f\"Loaded settings: {_settings.model_dump()}\")\n    return _settings\n\nsettings = get_settings()\n</code></pre>"},{"location":"dev-guide/web-apis/#model-validation","title":"Model Validation","text":"<ul> <li>Used for 'incoming' (user provided) data that needs to be validated.</li> </ul> <pre><code>from enum import Enum\nfrom pydantic import AnyUrl, BaseModel, EmailStr, Field, constr\n\nclass MusicBand(str, Enum):\n   AEROSMITH = \"AEROSMITH\"\n   QUEEN = \"QUEEN\"\n   ACDC = \"AC/DC\"\n\n\nclass UserBase(BaseModel):\n    first_name: str = Field(min_length=1, max_length=128)\n    username: constr(regex=\"^[A-Za-z0-9-_]+$\", to_lower=True, strip_whitespace=True)\n    email: EmailStr\n    age: int = Field(ge=18, default=None)  # must be greater or equal to 18\n    # only \"AEROSMITH\", \"QUEEN\", \"AC/DC\" values are allowed to be inputted\n    favorite_band: MusicBand = None\n    website: AnyUrl = None\n    valid_genre: Optional[boolean] = False\n\n    @field_validator(\"valid_genre\", mode=\"before\")\n    @classmethod\n    def get_genre_from_band_name(cls, value: Any, info: ValidationInfo) -&gt; str:\n        \"\"\"Get genre from band name.\"\"\"\n        if band := info.data.get(\"favorite_band\"):\n            log.debug(f\"Determining genre from band {band}\")\n            genre = band_genre_mapping(band)\n            if genre:\n                return True\n        return False\n</code></pre>"},{"location":"dev-guide/web-apis/#model-data-serialization","title":"Model Data Serialization","text":"<ul> <li>Used to format 'outgoing' data that is returned to a user.</li> </ul> <pre><code>class TaskBase(BaseModel):\n    \"\"\"Base Task model to inherit.\"\"\"\n    # ConfigDict has many options\n    # https://docs.pydantic.dev/latest/api/config/\n    # E.g. use_enum_values automatically runs .value on enums\n    # So a returned object will have\n    #   `somefield: 1`\n    # instead of\n    #   `somefield: SomeEnum.TYPE1`\n    model_config = ConfigDict(\n        use_enum_values=True,\n        validate_default=True,\n    )\n\n    # Exclude fields: for example we want to get these values from the database,\n    # and then process them into different fields in our returned model.\n    # outline (a WKB element from Postgis) --&gt; outline_geojson\n    outline: Any = Field(exclude=True)\n    lock_holder: Any = Field(exclude=True)\n\n    id: int\n    outline_geojson: Optional[Feature] = None\n    task_history: Optional[List[TaskHistoryBase]] = None\n\n\nclass TaskOut(TaskBase):\n    \"\"\"Task to return from endpoint.\"\"\"\n\n    locked_by_uid: Optional[int] = None\n    outline_geojson: Optional[int] = None\n\n    @field_serializer(\"locked_by_uid\")\n    def get_locked_by_uid(self, value: str) -&gt; str:\n        \"\"\"Get lock uid from lock_holder details.\"\"\"\n        if self.lock_holder:\n            return self.lock_holder.id\n        return None\n\n    @field_serializer(\"outline_geojson\")\n    def get_geojson_from_outline(self, value: Any, info: ValidationInfo) -&gt; str:\n        \"\"\"Get outline_geojson from Shapely geom.\"\"\"\n        if outline := info.data.get(\"outline\"):\n            properties = {\n                \"fid\": info.data.get(\"project_task_index\"),\n                \"uid\": info.data.get(\"id\"),\n                \"name\": info.data.get(\"project_task_name\"),\n            }\n            log.debug(\"Converting task outline to geojson\")\n            return geometry_to_geojson(outline, properties, info.data.get(\"id\"))\n        return None\n</code></pre>"},{"location":"dev-guide/web-apis/#response-models","title":"Response models","text":"<ul> <li>FastAPI integrates Pydantic very nicely.</li> <li>Endpoints allow us to define a <code>response_model</code>, which is a Pydantic model.</li> <li>This specifies the fields that must be present in the endpoint JSON response.</li> <li>Validators and serialisers are all called when a response_model is used.<ul> <li>This means that formatting and validation of the returned data does not   need to be done in the endpoint code.</li> <li>It is instead handled by Pydantic, and will throw an error if validation   does not pass.</li> </ul> </li> </ul> <p>Example:</p> <pre><code># project_schemas.py\nclass ProjectBase(BaseModel):\n    id: int\n    name: str\n\nclass ProjectInt(ProjectBase)\n    organization: str  # org abbreviation provided by frontend\n\n    @field_validator(\"organization\", mode=\"before\")\n    @classmethod\n    def get_org_long_name(cls, value: str) -&gt; str:\n        return get_org_long_name_from_abbreviation(value)\n\nclass ProjectOut(ProjectBase):\n    date_created: datetime.date\n\n    @field_serializer(\"date_created\")\n    def format_date(self, value: datetime.date):\n          # Format: Monday 01 2023\n          return last_active.strftime(\"%d %b %Y\")\n\n\n# project_routes.py\n@router.put(\"/{id}\", response_model=ProjectOut)\nasync def update_project(\n    id: int,\n    project_info: ProjectIn,\n    db: Session = Depends(database.get_db),\n):\n    \"\"\"Update an existing project by ID.\"\"\"\n    project = await project_crud.update_project_info(db, project_info, id)\n    if not project:\n        raise HTTPException(status_code=422, detail=\"Project update failed\")\n    return project\n</code></pre>"},{"location":"dev-guide/web-apis/#4-fastapi-dependencies-depends","title":"4. FastAPI Dependencies (Depends)","text":""},{"location":"dev-guide/web-apis/#validation-of-additional-constraints","title":"Validation of additional constraints","text":"<ul> <li>Pydantic can only validate the 'incoming' data from client input.</li> <li>Use dependencies (Depends) to validate input against other constraints:<ul> <li>Database constraints, such as project or email already exists, user not found.</li> <li>Auth constraints, where the users level of authorization should be assessed   in an endpoint.</li> </ul> </li> </ul> <p>Example:</p> <pre><code># logic.py (where the dependency is written)\nasync def valid_post_id(post_id: UUID4) -&gt; Mapping:\n    post = await service.get_by_id(post_id)\n    if not post:\n        raise PostNotFound()\n\n    return post\n\n\n# routes.py (where Depends is used)\n@router.get(\"/posts/{post_id}\", response_model=PostResponse)\nasync def get_post_by_id(post: Mapping = Depends(valid_post_id)):\n    return post\n\n\n@router.put(\"/posts/{post_id}\", response_model=PostResponse)\nasync def update_post(\n    update_data: PostUpdate,\n    post: Mapping = Depends(valid_post_id),\n):\n    updated_post: Mapping = await service.update(id=post[\"id\"], data=update_data)\n    return updated_post\n</code></pre> <p>If we didn't put data validation in a dependency, we would have to do the same checks for on each endpoint (duplicating code).</p>"},{"location":"dev-guide/web-apis/#reuse-chain-dependencies","title":"Reuse &amp; chain dependencies","text":"<ul> <li>Dependencies can use other dependencies and repeating code.</li> </ul> <p>Example:</p> <pre><code># logic.py\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import JWTError, jwt\n\n# Depends on pre-existing FastAPI dependency OAuth2PasswordBearer\nasync def parse_jwt_data(\n    token: str = Depends(OAuth2PasswordBearer(tokenUrl=\"/auth/token\"))\n) -&gt; dict:\n    try:\n        payload = jwt.decode(token, \"JWT_SECRET\", algorithms=[\"HS256\"])\n    except JWTError:\n        raise InvalidCredentials()\n\n    return {\"user_id\": payload[\"id\"]}\n\n# Depends on parse_jwt_data (chained)\nasync def valid_owned_post(\n    post: Mapping = Depends(valid_post_id),\n    token_data: dict = Depends(parse_jwt_data),\n) -&gt; Mapping:\n    if post[\"creator_id\"] != token_data[\"user_id\"]:\n        raise UserNotOwner()\n\n    return post\n\n# routes.py (where the final Depends is used)\n@router.get(\"/users/{user_id}/posts/{post_id}\", response_model=PostResponse)\nasync def get_user_post(post: Mapping = Depends(valid_owned_post)):\n    return post\n</code></pre>"},{"location":"dev-guide/web-apis/#dependency-call-are-cached","title":"Dependency call are cached","text":"<ul> <li>Dependencies can be reused multiple times, and they won't be recalculated.</li> <li>FastAPI caches dependency's result within a request's scope by default:<ul> <li>If a dependency makes a DB call, this can be cached when the dependency is   called again.</li> <li>With this in mind, try to de-couple dependencies, i.e. write smaller   functions that do specific things, then chain them.</li> </ul> </li> </ul> <p>Example:</p> <pre><code># logic.py (contains dependencies here)\nfrom fastapi import BackgroundTasks\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import JWTError, jwt\n\n\n# Dependency 1\nasync def valid_post_id(post_id: UUID4) -&gt; Mapping:\n    post = await service.get_by_id(post_id)\n    if not post:\n        raise PostNotFound()\n\n    return post\n\n# Dependency 2\nasync def parse_jwt_data(\n    token: str = Depends(OAuth2PasswordBearer(tokenUrl=\"/auth/token\"))\n) -&gt; dict:\n    try:\n        payload = jwt.decode(token, \"JWT_SECRET\", algorithms=[\"HS256\"])\n    except JWTError:\n        raise InvalidCredentials()\n\n    return {\"user_id\": payload[\"id\"]}\n\n# Dependency 3 uses both 1 &amp; 2\nasync def valid_owned_post(\n    post: Mapping = Depends(valid_post_id),\n    token_data: dict = Depends(parse_jwt_data),\n) -&gt; Mapping:\n    if post[\"creator_id\"] != token_data[\"user_id\"]:\n        raise UserNotOwner()\n\n    return post\n\n# Dependency 4 also uses dependency 2 (and is cached)\nasync def valid_active_creator(\n    token_data: dict = Depends(parse_jwt_data),\n):\n    user = await users_service.get_by_id(token_data[\"user_id\"])\n    if not user[\"is_active\"]:\n        raise UserIsBanned()\n\n    if not user[\"is_creator\"]:\n       raise UserNotCreator()\n\n    return user\n\n\n# routes.py (uses both Dependency 3 &amp; 4)\n@router.get(\"/users/{user_id}/posts/{post_id}\", response_model=PostResponse)\nasync def get_user_post(\n    worker: BackgroundTasks,\n    post: Mapping = Depends(valid_owned_post),\n    user: Mapping = Depends(valid_active_creator),\n):\n    \"\"\"Get post that belong the active user.\"\"\"\n    worker.add_task(notifications_service.send_email, user[\"id\"])\n    return post\n</code></pre>"},{"location":"dev-guide/web-apis/#dependencies-can-include-route-parameters","title":"Dependencies can include route parameters","text":"<ul> <li>Sometimes a dependency requires additional variables to run it's logic.</li> <li>As an example we can imagine an app that has users and projects:<ul> <li>To determine if a user has permission to access a project we need both:<ul> <li>The user id</li> <li>The project id</li> </ul> </li> <li>The user id could be determined via another dependency.</li> <li>However, the project id must be passed in by the user.</li> </ul> </li> </ul> <pre><code># logic.py (where dependencies are located)\n\nfrom app.auth.osm import AuthUser, login_required # imported dependency\n\nasync def validator(\n    project_id: int, # The route parameter\n    db: Session = Depends(get_db),\n    user_data: AuthUser = Depends(login_required), # from imported dependency\n) -&gt; AuthUser:\n    user_id = await get_uid(user_data)\n\n    match = (\n        db.query(DbUserRoles).filter_by(user_id=user_id, project_id=project_id).first()\n    )\n\n    if not match:\n        raise HTTPException(status_code=403, detail=\"User has no access to project\")\n\n    if match.role.value &lt; ProjectRole.VALIDATOR.value:\n        raise HTTPException(\n            status_code=403, detail=\"User is not a validator for this project\"\n        )\n\n    return user_data\n\n# routes.py (endpoints)\n@router.get(\"/get_validator/\")\nasync def validator(\n    db: Session = Depends(database.get_db),\n    user: AuthUser = Depends(validator),\n):\n    return user\n</code></pre> <p>When the user calls the <code>/get_validator</code> endpoint, they will need to provide the parameter <code>project_id</code>, as it is present in the <code>validator</code> sub dependency.</p>"},{"location":"dev-guide/web-apis/#5-always-use-typing","title":"5. Always Use Typing","text":"<ul> <li>FastAPI relies on Typing heavily for it's functionality.</li> <li>Typing also helps linting and IDE code completion.</li> <li>Pydantic models can be used as types.</li> <li>If endpoints often reference data in the same format, it's useful to have a model.</li> </ul> <p>For example an authenticated user model:</p> <pre><code>class AuthUser(BaseModel):\n    id: int\n    username: str\n    img_url: Optional[str]\n\n# Usage\nuser: AuthUser = get_auth_user()\n</code></pre>"},{"location":"dev-guide/web-apis/#6-use-rest-endpoint-naming","title":"6. Use REST Endpoint Naming","text":"<p>REST APIs are formatted as such:</p> <pre><code>GET /projects/:project_id\nGET /projects/:project_id/tasks/:task_id/submissions\nGET /users/:user_id\n</code></pre> <p>In summary:</p> <ul> <li><code>projects</code> is the noun in this example.</li> <li>Always use plural nouns: <code>projects/xxx</code> vs <code>project/xxx</code>.</li> <li>Never use verbs in endpoint: <code>projects/11/create</code><ul> <li>Instead use GET, POST, PUT, PATCH, DELETE methods.</li> </ul> </li> </ul> <p>It is also recommended to add a version, e.g. <code>/v2/projects</code>, to the API.</p> <p>However, if the project is small, this may not always be necessary.</p>"},{"location":"dev-guide/web-apis/#7-save-files-in-chunks","title":"7. Save Files in Chunks","text":"<ul> <li>If the API needs to receive a large file from a user, receive it in chunks:</li> </ul> <pre><code>import aiofiles\nfrom fastapi import UploadFile\n\nDEFAULT_CHUNK_SIZE = 1024 * 1024 * 50  # 50 megabytes\n\nasync def save_video(video_file: UploadFile):\n   async with aiofiles.open(\"/file/path/name.mp4\", \"wb\") as f:\n     while chunk := await video_file.read(DEFAULT_CHUNK_SIZE):\n         await f.write(chunk)\n</code></pre>"},{"location":"dev-guide/web-frameworks/","title":"JavaScript Frameworks","text":""},{"location":"dev-guide/web-frameworks/#ssg-spa-ssr-etc","title":"SSG, SPA, SSR, etc","text":"<p>TODO</p>"},{"location":"dev-guide/web-frameworks/#frameworks","title":"Frameworks","text":""},{"location":"dev-guide/web-frameworks/#react","title":"React","text":"<p>As of 2023, it is no longer recommended to design new applications using React.</p> <p>React is not going anywhere soon, with many experienced developers in the field and a lot of technical debt accumulated by organizations.</p> <p>An interesting article on this topic can be found here.</p>"},{"location":"dev-guide/web-frameworks/#web-components","title":"Web Components","text":"<p>TODO</p>"},{"location":"modules/backend/","title":"Backend Python Modules","text":""},{"location":"modules/backend/#osm-login-python","title":"OSM Login Python","text":"<p>A way to consistently implement OSM login (via underlying OAuth2) in our applications.</p>"},{"location":"modules/backend/#tm-extractor","title":"TM Extractor","text":"<p>The TM Extractor script is designed to trigger extraction requests for Tasking Manager projects.</p>"},{"location":"modules/backend/#osm-raw-data","title":"OSM Raw Data","text":"<p>OMS Raw Data supports the creation of data extracts into GeoJson or a Postgres database. There is more detail on these modules on the project website).</p>"},{"location":"modules/backend/#postgresclient","title":"PostgresClient()","text":"<p>This program extracts data from a local postgres data, or the remote Underpass one. A boundary polygon is used to define the area to be covered in the extract.</p>"},{"location":"modules/backend/#mapimporter","title":"MapImporter()","text":"<p>Imports data into a postgres database that is using the Underpass schema. This currently supports three input formats, Geojson, all OSM formats, and Parquet files from Overture.</p>"},{"location":"modules/backend/#queryconfig","title":"QueryConfig()","text":"<p>This reads a config file in YAML format that describes the database query. This is used to generate the queries for the different database sources.</p>"},{"location":"modules/backend/#overture","title":"Overture()","text":"<p>Parses Overture Parquet files and creates a data structure of the data so it can be imported into postgres, or generate an output GeoJson file.</p>"},{"location":"modules/backend/#osm-fieldwork","title":"OSM Fieldwork","text":"<p>Various utility programs useful for field data collection using ODK Collect. These modules are used extensively in FMTM for all the backend data processing.</p> <p>There is more detail on these modules on the project website).</p>"},{"location":"modules/backend/#basemapper","title":"BaseMapper()","text":"<p>This makes basemaps of satellite imagery for ODK Collect and OsmAnd.</p>"},{"location":"modules/backend/#csvdump","title":"CSVDump()","text":"<p>Converts the CSV files downloaded from ODK Central into OSM XML and GeoJson so they can be edited in JOSM or QGIS.</p>"},{"location":"modules/backend/#jsondump","title":"JsonDump()","text":"<p>Converts the JSON files downloaded from ODK Central into OSM XML and GeoJson so they can be edited in JOSM or QGIS.</p>"},{"location":"modules/backend/#filterdata","title":"FilterData()","text":"<p>This scans an XLSForm, and removes all features from the data extract that are not in the choices sheet. This is required to use the data extract in ODK Collect.</p>"},{"location":"modules/backend/#makeextract","title":"MakeExtract()","text":"<p>For ODK Collect, this makes the filtered data extract using osm-rawdata and FilterData().</p>"},{"location":"modules/backend/#odkcentral","title":"OdkCentral()","text":"<p>This supports working with the REST API for ODK Central.</p>"},{"location":"modules/backend/#osmfile","title":"OsmFile()","text":"<p>Produces OSM XML out files.</p>"},{"location":"modules/backend/#fmtmsplitter","title":"FMTMSplitter","text":"<p>A splitting algorithm using PostGIS to divide an idea into task areas, factoring in prominent map features (roads, rivers, etc).</p> <p>The division is configurable via various parameters.</p>"},{"location":"modules/backend/#fair-utilities","title":"fAIr Utilities","text":"<p>Various machine learning utils used within the fAIr backend.</p>"},{"location":"modules/backend/#conflator","title":"Conflator","text":"<p>This is a project for conflating external data sets with OpenStreetMap data.</p> <p>There is more detail on these modules on the project website</p>"},{"location":"modules/backend/#conflatebuildings","title":"ConflateBuildings()","text":"<p>Conflate buildings from external datasets like the Microsoft or Google building footprints.</p>"},{"location":"modules/backend/#conflatepoi","title":"ConflatePOI()","text":"<p>Conflate POIs from external datasets like ODK Collect.</p>"},{"location":"modules/backend/#tm-admin","title":"TM Admin","text":"<p>This is a project for profile management of Tasking Manager style projects.</p>"},{"location":"modules/backend/#tmadminmanage","title":"TmAdminManage","text":"<p>This class and utility program handles creating the data type files, and the database tables. It also support migration as the database schema changes.</p>"},{"location":"modules/backend/#dbsupport","title":"DBSupport","text":"<p>Base class for database functions, used by the generated wrapper classes.</p>"},{"location":"modules/backend/#generator","title":"Generator","text":"<p>This class reads in the YAML config file for a database table, and generates the protobuf files, SQL files, and Python class.</p>"},{"location":"modules/backend/#protobuf","title":"ProtoBuf","text":"<p>Used by generator for protobuf specific file creation.</p>"},{"location":"modules/backend/#wrapper-classes","title":"Wrapper Classes","text":"<p>These wrapper classes are generated from the YAML config file, and contain all the columns from the database table as a Python class. This is used to create, update, and query the database tables.</p> <ul> <li>UsersDB()</li> <li>TeamsDB()</li> <li>ProjectsDB()</li> <li>TasksDB()</li> <li>OrganizationsDB()</li> </ul>"},{"location":"modules/frontend/","title":"Frontend Modularization","text":""},{"location":"modules/frontend/#shared-ui-components","title":"Shared UI Components","text":"<p>Consistent styling and component functionality across all of our projects.</p> <p>Currently in design and prototype stage, but will be rolled out for all projects eventually.</p> <p>link</p>"},{"location":"modules/frontend/#shared-map-components","title":"Shared Map Components","text":"<p>Map components using OpenLayers underneath.</p> <p>Currently in design and prototype stage, but will be rolled out for all projects eventually.</p> <p>Will likely use PlanetLab's Maps at it's core, with fleshed out higher level components:</p> <ul> <li>Layer switcher</li> <li>Attribute table</li> <li>Various layer types</li> </ul> <p>The plan would be to support various cloud optimised geo data formats:</p> <ul> <li>COG</li> <li>flatgeobuf</li> <li>GeoParquet</li> <li>PMTiles</li> </ul> <p>There is also the possibility of designing this with Svelte, then embedding the compiled plain JS into our React apps (in discussion).</p> <p>link</p>"},{"location":"modules/frontend/#qr-codes","title":"QR Codes","text":"<p>Reads and decodes base64 and zlib encoded JSON data within a QRCode in Javascript</p>"},{"location":"techdoc/","title":"HOT Documentation","text":"<p>Project-specific documentation can be found in the docs folder for the project. This is the home of everything that does not belong to one specific project.</p> <p>Here you can find:</p>"},{"location":"techdoc/#learn-about-the-hot-documentation-approach","title":"Learn about the HOT Documentation Approach","text":""},{"location":"techdoc/#technical-documentation-templates","title":"Technical Documentation Templates","text":"<p>Standard documentation templates for HOT technical documentation.</p>"},{"location":"techdoc/#learn-about-the-hot-architecture","title":"Learn About the HOT Architecture","text":""},{"location":"techdoc/#overarching-views-of-the-architecture","title":"Overarching Views of the Architecture","text":"<p>Architecture views (and other documentation) that is not specific to one HOTSM application and applicable across multiple or all.</p>"},{"location":"techdoc/#the-architecture-decision-log","title":"The Architecture Decision Log","text":"<p>A register of all the key architecture decisions for the HOTSM ecosystem, including some detail around why each was decided.</p>"},{"location":"techdoc/#risk-log","title":"Risk Log","text":"<p>A register of all the overarching risks for the HOTSM ecosystem.</p> <p>You can visit the Humanitarian OpenStreetMap Team at https://github.com/hotosm and check out our pinned projects or browse through the full family of HOTSM projects.</p>"},{"location":"techdoc/overview/","title":"Overviews","text":"<p>While today HOT architecture consists of a wide range of applications and modules, the vision for the future is a more unified ecosystem.</p> <p>The key features of this architecture are: * An integrated set of back-end modules * A common data model based on standards and only extending as   required * Fit for purpose front-ends, built on REACT</p>"},{"location":"techdoc/overview/#hot-overview","title":"HOT Overview","text":"<p>This is an overarching view of the HOT ecosystem of solutions. It includes some key non-HOT components for context and because they serve an important role in the ecosystem.</p> <p></p>"},{"location":"techdoc/overview/#solution-user-view","title":"Solution User View","text":"<p>This solution user view provides a user-centric view of the HOT architecture showing the key user roles and what activities they will perform using the solution. You can learn more about the diagram and notation here.</p> <p></p>"},{"location":"techdoc/overview/#information-flow","title":"Information Flow","text":"<p>This information flow is a dynamic view that shows the flow of information (in high level business terms) between the HOT ecosystem components. You can learn more about the diagram and notation here.</p> <p></p>"},{"location":"techdoc/overview/#component-model","title":"Component Model","text":"<p>This component model is a static view that shows how the HOT ecosystem components connect to one another. You can learn more about the diagram and notation here.</p> <p></p>"},{"location":"techdoc/overview/#end-to-end-e2e","title":"End-To-End (E2E)","text":"<p>This is a plan, currently in progress, to have a better end user experience between multiple HOT projects. Since often multiple projects are used, Tasking Manager, Export Tool, Field Mapping Tasking Manager, etc... the journey between them should be efficient.</p>"},{"location":"techdoc/overview/#e2e-data-integration-diagrams","title":"E2E Data Integration Diagrams","text":"<p>This is the high level concept for the E2E data integration approach. The TD Admin components in the center are not actually a hub instance, but a set of shared modules used for connectivity.  </p>"},{"location":"techdoc/overview/#e2e-integration-sequence-diagrams","title":"E2E Integration Sequence Diagrams","text":"<p>These show the interactions between multiple components at a more detailed level.</p> <p>|  |  | |  | . |</p>"},{"location":"techdoc/overview/#e2e-conceptual-data-model","title":"E2E Conceptual Data Model","text":"<p>This is a conceptual data model, illustrating the key entities and relationship in the data model. </p>"},{"location":"techdoc/templates/","title":"HOT Documentation Templates","text":"<p>These are templates that can be used to document HOT's tech projects.</p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"techdoc/templates/Decisions/","title":"Decisions","text":"<p>All italics is helper text. Delete when filling out. More guidance on decisions here: https://wittij.com/solution-architecture-decisions</p>"},{"location":"techdoc/templates/Decisions/#state-the-decision-here","title":"State the Decision Here","text":"<p>The decision should be stated as the top level heading; as an assertion: \"Front ends will be built using REACT.\"</p>"},{"location":"techdoc/templates/Decisions/#analysis-options-considered","title":"Analysis / Options Considered","text":"<p>Summary of analysis leading to the decision. Should include options considered, pros and cons to each, and explanation of why this was decided. Try to minimize long narrative - prefer use of structured text (bullets and tables) and illustrations instead. Can also link to an external options analysis. See more on options analysis here: https://wittij.com/solution-options-analysis</p>"},{"location":"techdoc/templates/Decisions/#related-information","title":"Related Information","text":"<p>Links to external information that is of use to this decision</p>"},{"location":"techdoc/templates/Decisions/#bad-decision","title":"Bad Decision","text":"<p>This section is only completed if the decision is know to not be the right long term one.</p>"},{"location":"techdoc/templates/Decisions/#ideal-target","title":"Ideal Target","text":"<p>What the decision should have been if this is not it!</p>"},{"location":"techdoc/templates/Decisions/#factors","title":"Factors","text":"<p>Most typical factors are not enough funding, not enough time, or lack of capabilities from a component of the architecture</p>"},{"location":"techdoc/templates/Risks/","title":"Risk is stated Here","text":""},{"location":"techdoc/templates/Risks/#impact","title":"Impact","text":"<p>Narrative explaining what bad will happen if this risk happens.</p>"},{"location":"techdoc/templates/Risks/#severity","title":"Severity","text":"<p>Hoe bad will it be? Severity of the risk on this scale: negligible, marginal, critical, catastrophic.</p>"},{"location":"techdoc/templates/Risks/#likelihood","title":"Likelihood","text":"<p>How possible is it that this happens? Likelihood of the risk on this scale: rare, unlikely, possible, likely, certain.</p>"},{"location":"techdoc/templates/Risks/#response","title":"Response","text":"<p>Narrative explanation of how this risk will be addressed. Will be a combination of how the risk will be managed. Some common approaches are: mitigation, monitoring, prepared a response, contracting to another party.</p>"}]}